{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=256, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.f = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        prev = F.pad(xn, (0, 0, 1, -1))\n",
    "        prev2 = F.pad(xn, (0, 0, 2, -2))\n",
    "        f = torch.sigmoid(self.f(prev)) # prev can say what should be forgotten from prev2 (x-2)\n",
    "        q = torch.sigmoid(self.q(xn)) # what should be accepted from prev\n",
    "        v = self.v(prev) # what the prev is providing\n",
    "        attn = self.proj((q * v) - (f * self.v(prev2)))\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 439.70it/s]\n",
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 207 K \n",
      "-------------------------------\n",
      "207 K     Trainable params\n",
      "0         Non-trainable params\n",
      "207 K     Total params\n",
      "0.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 27.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And nowO:i.iGAN&yJFRt!XWxt!seFrxbXqh. WWjkjwoeiyJGH!EnzNBU.O:3Opext3woh\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 202/202 [00:07<00:00, 27.95it/s, loss=1.6, v_num=68] And now,\n",
      "But your libted frues me for I have but but her treach your ba\n",
      "Epoch 1: 100%|██████████| 202/202 [00:06<00:00, 30.68it/s, loss=1.47, v_num=68, test_loss=1.770]And now, friends in proud, tillffer upon; seel tears mine.\n",
      "\n",
      "ANGELO:\n",
      "The\n",
      "Epoch 2: 100%|██████████| 202/202 [00:06<00:00, 31.13it/s, loss=1.4, v_num=68, test_loss=1.560] And now?\n",
      "\n",
      "CAMILLO:\n",
      "Ay, sun my life,\n",
      "But is the demany love of my hound \n",
      "Epoch 3: 100%|██████████| 202/202 [00:06<00:00, 31.41it/s, loss=1.38, v_num=68, test_loss=1.500]And now, a set it bates, or barks a word you been his lady's born sound\n",
      "Epoch 4: 100%|██████████| 202/202 [00:06<00:00, 31.19it/s, loss=1.35, v_num=68, test_loss=1.480]And now turn then the doing thy sease.\n",
      "\n",
      "Second Servingman one and taken\n",
      "Epoch 5: 100%|██████████| 202/202 [00:06<00:00, 31.01it/s, loss=1.34, v_num=68, test_loss=1.440]And now, it would to his plichers of this so faint.\n",
      "\n",
      "AUFIDIUS:\n",
      "My such \n",
      "Epoch 6: 100%|██████████| 202/202 [00:06<00:00, 30.68it/s, loss=1.32, v_num=68, test_loss=1.450]And now to her absent daughter, thou shouldst thou not to's honour and \n",
      "Epoch 7: 100%|██████████| 202/202 [00:06<00:00, 30.18it/s, loss=1.31, v_num=68, test_loss=1.430]And now! believe me for off an admy heart is banish'd to absor as where\n",
      "Epoch 8: 100%|██████████| 202/202 [00:06<00:00, 30.06it/s, loss=1.31, v_num=68, test_loss=1.450]And now to his love!\n",
      "Have too. Let he shall forget them become is to li\n",
      "Epoch 9: 100%|██████████| 202/202 [00:06<00:00, 29.96it/s, loss=1.29, v_num=68, test_loss=1.450]And now make a man; and your hand;\n",
      "And I command would more three my de\n",
      "Epoch 10: 100%|██████████| 202/202 [00:06<00:00, 29.55it/s, loss=1.28, v_num=68, test_loss=1.420]And now you too?\n",
      "\n",
      "FRIAR POMnO:\n",
      "The better fool strike the steers o' the\n",
      "Epoch 11: 100%|██████████| 202/202 [00:06<00:00, 29.35it/s, loss=1.27, v_num=68, test_loss=1.420]And now! soon council,\n",
      "And caped forth.\n",
      "\n",
      "LUCIO:\n",
      "Tut, I crack'd me, we a\n",
      "Epoch 12: 100%|██████████| 202/202 [00:06<00:00, 28.96it/s, loss=1.27, v_num=68, test_loss=1.400]And now to call you?\n",
      "Olarded till set about your sir;\n",
      "I'll stay,\n",
      "That m\n",
      "Epoch 13: 100%|██████████| 202/202 [00:06<00:00, 29.24it/s, loss=1.26, v_num=68, test_loss=1.410]And now\n",
      "Faith, let us sin\n",
      "Wherein it is say' notorious life,' quoth wri\n",
      "Epoch 14: 100%|██████████| 202/202 [00:07<00:00, 28.56it/s, loss=1.25, v_num=68, test_loss=1.390]And now--\n",
      "By pardon.\n",
      "\n",
      "MARCIUS:\n",
      "The watch you, a man sets.\n",
      "\n",
      "CORIOLANUS:\n",
      "\n",
      "Epoch 15: 100%|██████████| 202/202 [00:07<00:00, 26.49it/s, loss=1.25, v_num=68, test_loss=1.410]And now in the side,\n",
      "Where he will make wrongs,\n",
      "And as it our former ni\n",
      "Epoch 16: 100%|██████████| 202/202 [00:07<00:00, 25.48it/s, loss=1.24, v_num=68, test_loss=1.420]And now, well as it should not, mart in the world thou hast awhile! wha\n",
      "Epoch 17: 100%|██████████| 202/202 [00:09<00:00, 22.40it/s, loss=1.23, v_num=68, test_loss=1.400]And now thinks had that stands were he shall have mortal pity.\n",
      "When you\n",
      "Epoch 18: 100%|██████████| 202/202 [00:11<00:00, 17.55it/s, loss=1.23, v_num=68, test_loss=1.410]And now nothing seems. Therefore he will we hear no lips,\n",
      "Were not to s\n",
      "Epoch 19: 100%|██████████| 202/202 [00:12<00:00, 16.54it/s, loss=1.22, v_num=68, test_loss=1.410]And now he hath starve for me?\n",
      "\n",
      "First Lady:\n",
      "I'll give his counsel is pl\n",
      "Epoch 20: 100%|██████████| 202/202 [00:12<00:00, 15.85it/s, loss=1.22, v_num=68, test_loss=1.410]And now dost thou conclane of care,\n",
      "And to see him so he that brokel cr\n",
      "Epoch 21: 100%|██████████| 202/202 [00:13<00:00, 15.18it/s, loss=1.22, v_num=68, test_loss=1.390]And now I thought of length,\n",
      "Whose thousand presently at the first,\n",
      "And\n",
      "Epoch 22: 100%|██████████| 202/202 [00:11<00:00, 17.17it/s, loss=1.21, v_num=68, test_loss=1.400]And now I think, as you before\n",
      "Burning of worthy and deserve them the p\n",
      "Epoch 23: 100%|██████████| 202/202 [00:12<00:00, 16.61it/s, loss=1.2, v_num=68, test_loss=1.380] And now, the slips and like one hath suck\n",
      "But the king and a match; and\n",
      "Epoch 24: 100%|██████████| 202/202 [00:11<00:00, 17.38it/s, loss=1.2, v_num=68, test_loss=1.380] And now will have at less curse\n",
      "As sailor thy sadness.\n",
      "\n",
      "SAMPSON:\n",
      "Are th\n",
      "Epoch 24: 100%|██████████| 202/202 [00:12<00:00, 16.15it/s, loss=1.2, v_num=68, test_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 202/202 [00:12<00:00, 16.10it/s, loss=1.2, v_num=68, test_loss=1.410]\n"
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "from shared import corpus, tokenizers, trainers\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = GPT(tokenizer.get_vocab_size())\n",
    "trainer = trainers.CausalTrainer(model, tokenizer, device = \"mps\")\n",
    "trainer.train(text, batch_size=36, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "\n",
      "First Senator:\n",
      "If you like.\n",
      "\n",
      "First Hunger than your family! mark the worst of dark;\n",
      "I came infrink, madam, farewell.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "My dearest of his people to his heart the time?\n",
      "What still, I have such perfection, the most breathe forest bid him so breathize some hurt as none miles of damned at the harms and famous and play.\n",
      "A begging of ill.\n",
      "\n",
      "MARCIUS:\n",
      "'Tyition:\n",
      "Truly son\n",
      "He had shed and down, and not so, because\n",
      "He does arriving my princely good friends against the glorious prince you have not stir seem into\n",
      "Supply she shall show your gentleman:\n",
      "It more perfect the princely good as his face?\n",
      "\n",
      "First Murderer:\n",
      "I do proved him, with an o\n"
     ]
    }
   ],
   "source": [
    "print(trainer.wrapper.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
