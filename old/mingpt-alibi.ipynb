{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=256, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(block_size, embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, block_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size, dtype=torch.bool)))\n",
    "        t = torch.arange(-block_size+1, 1)\n",
    "        self.register_buffer(\"alibi\", torch.tril(torch.cat([t, t[:-1]]).unfold(0, len(t), 1).flip(0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(self.ln1(x)).chunk(3, dim=-1)\n",
    "        w = q @ k.transpose(-2, -1) / math.sqrt(C) # (B, T, T)\n",
    "        w = w + self.alibi[:T, :T]\n",
    "        w.masked_fill_(self.mask[:T, :T], float(\"-Inf\"))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        attn = self.proj(w @ v) # (B, T, C)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 602.80it/s]\n",
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 207 K \n",
      "-------------------------------\n",
      "207 K     Trainable params\n",
      "0         Non-trainable params\n",
      "207 K     Total params\n",
      "0.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]And now;ryy,x;x$ q-Q I;BpXMcNJrkSmTb&F-qKb\n",
      " bd3Bl&FNv-$Is?pXxazZJBCT b-\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 202/202 [00:10<00:00, 19.15it/s, loss=1.75, v_num=70]And now,\n",
      "The beart the two met while the dangue.\n",
      "Ingeat and turn. Marem\n",
      "Epoch 1: 100%|██████████| 202/202 [00:07<00:00, 26.66it/s, loss=1.57, v_num=70, test_loss=1.890]And now,\n",
      "Again, you are the were sir, a sir, in, she before of Geaver a\n",
      "Epoch 2: 100%|██████████| 202/202 [00:07<00:00, 26.76it/s, loss=1.49, v_num=70, test_loss=1.720]And now,\n",
      "As show me been\n",
      "Wherei wit thyself a prisons and mine a charts\n",
      "Epoch 3: 100%|██████████| 202/202 [00:07<00:00, 26.77it/s, loss=1.44, v_num=70, test_loss=1.600]And now and speak and souls, tender of the wingle is and at thou,\n",
      "Or yo\n",
      "Epoch 4: 100%|██████████| 202/202 [00:07<00:00, 26.78it/s, loss=1.41, v_num=70, test_loss=1.530]And now about in the grows not bie speak'st blaze, far turn his blood l\n",
      "Epoch 5: 100%|██████████| 202/202 [00:07<00:00, 26.47it/s, loss=1.39, v_num=70, test_loss=1.540]And now which bried,\n",
      "For the times fair\n",
      "But be so murder and appearer-s\n",
      "Epoch 6: 100%|██████████| 202/202 [00:07<00:00, 26.76it/s, loss=1.38, v_num=70, test_loss=1.490]And now.\n",
      "Win my seat most a minures to\n",
      "so fearing his true\n",
      "The print he\n",
      "Epoch 7: 100%|██████████| 202/202 [00:07<00:00, 26.60it/s, loss=1.36, v_num=70, test_loss=1.480]And now misger to turn and her passion,\n",
      "Donest, we\n",
      "canding have.\n",
      "\n",
      "LEONT\n",
      "Epoch 8: 100%|██████████| 202/202 [00:07<00:00, 26.56it/s, loss=1.34, v_num=70, test_loss=1.430]And now:\n",
      "I am honour'st was speak,\n",
      "Which way to leave your peter of wha\n",
      "Epoch 9: 100%|██████████| 202/202 [00:07<00:00, 26.22it/s, loss=1.34, v_num=70, test_loss=1.460]And now.\n",
      "\n",
      "BLEONTES:\n",
      "O, now that your death; shall I see.\n",
      "\n",
      "BENVOLIO:\n",
      "I d\n",
      "Epoch 10: 100%|██████████| 202/202 [00:07<00:00, 25.81it/s, loss=1.33, v_num=70, test_loss=1.450]And now I never:\n",
      "The consul, and a further of this maiding,\n",
      "Nor for me.\n",
      "Epoch 11: 100%|██████████| 202/202 [00:07<00:00, 25.30it/s, loss=1.32, v_num=70, test_loss=1.420]And now awake you and cried\n",
      "Better together.\n",
      "\n",
      "CAPULET:\n",
      "A powbring of my\n",
      "Epoch 12: 100%|██████████| 202/202 [00:08<00:00, 24.58it/s, loss=1.31, v_num=70, test_loss=1.430]And now, shall have many wench at the pride.\n",
      "\n",
      "CAPULET:\n",
      "He, my land to m\n",
      "Epoch 13: 100%|██████████| 202/202 [00:08<00:00, 23.96it/s, loss=1.31, v_num=70, test_loss=1.430]And now will have been storm!\n",
      "The plaints of my contemently humble beha\n",
      "Epoch 14: 100%|██████████| 202/202 [00:08<00:00, 23.45it/s, loss=1.31, v_num=70, test_loss=1.380]And now I'll be pleal too seen and as he had and she to her conjunction\n",
      "Epoch 15: 100%|██████████| 202/202 [00:08<00:00, 23.36it/s, loss=1.29, v_num=70, test_loss=1.410]And now.\n",
      "To make you a strength of his son\n",
      "Both him a strange, I say'd\n",
      "\n",
      "Epoch 16: 100%|██████████| 202/202 [00:08<00:00, 22.68it/s, loss=1.28, v_num=70, test_loss=1.400]And now fall woe,\n",
      "Hereford in drunk in all? it that I speak all my soul\n",
      "Epoch 17: 100%|██████████| 202/202 [00:09<00:00, 22.32it/s, loss=1.28, v_num=70, test_loss=1.390]And now I be but some shepherds:\n",
      "Trust all with that they whose apothec\n",
      "Epoch 18: 100%|██████████| 202/202 [00:09<00:00, 21.96it/s, loss=1.27, v_num=70, test_loss=1.420]And now, fair died,\n",
      "For sour had them my foes, but that sad should swee\n",
      "Epoch 19: 100%|██████████| 202/202 [00:09<00:00, 22.16it/s, loss=1.26, v_num=70, test_loss=1.380]And now he,\n",
      "Take me hath farewell; I beg at my meatter thee\n",
      "It is truth\n",
      "Epoch 20: 100%|██████████| 202/202 [00:09<00:00, 21.72it/s, loss=1.27, v_num=70, test_loss=1.380]And now his honour we are.\n",
      "\n",
      "AUFIDIUS:\n",
      "Mark this here I was a fast?\n",
      "\n",
      "BRU\n",
      "Epoch 21: 100%|██████████| 202/202 [00:09<00:00, 21.55it/s, loss=1.26, v_num=70, test_loss=1.390]And now had bones to plagued;\n",
      "Or not meet flikeness for all the man,\n",
      "An\n",
      "Epoch 22: 100%|██████████| 202/202 [00:09<00:00, 21.78it/s, loss=1.25, v_num=70, test_loss=1.390]And now, and be shride--weed foul two word with their penitent accompan\n",
      "Epoch 23: 100%|██████████| 202/202 [00:09<00:00, 21.60it/s, loss=1.25, v_num=70, test_loss=1.370]And now no more heart\n",
      "That the hours and hangman a kind of hostam,\n",
      "O, t\n",
      "Epoch 24: 100%|██████████| 202/202 [00:09<00:00, 21.47it/s, loss=1.25, v_num=70, test_loss=1.400]And now myself of the title heart is, and the people's worthy more to t\n",
      "Epoch 24: 100%|██████████| 202/202 [00:10<00:00, 19.82it/s, loss=1.25, v_num=70, test_loss=1.400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 202/202 [00:10<00:00, 19.75it/s, loss=1.25, v_num=70, test_loss=1.400]\n"
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "from shared import corpus, tokenizers, trainers\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = GPT(tokenizer.get_vocab_size())\n",
    "trainer = trainers.CausalTrainer(model, tokenizer, device = \"mps\")\n",
    "trainer.train(text, batch_size=36, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! what wicked the more to so time to mribunes.\n",
      "If thou mayor her half their bones in hope,\n",
      "You stand and with those could be thy cause away:\n",
      "Nay, were as my heart.\n",
      "\n",
      "CATESBY:\n",
      "Yet, thou hap to call the will heart, in stand here in backs.\n",
      "Have you to comfort, I shall be my honour,\n",
      "Such shall be put to the thirst nost make us hanging weight defend home\n",
      "It in the multies.\n",
      "This, my lord, and, a month the whole my face;\n",
      "Sailorous sure these moon the gates their sweet banish'd in a word.\n",
      "\n",
      "MENENIUS:\n",
      "O, misery have made for any\n",
      "A distress safety to hear\n",
      "My point, while at Plantagenet; so I'll say, is so honour:\n",
      "No, then come\n",
      "the prison is the clouds, sh\n"
     ]
    }
   ],
   "source": [
    "print(trainer.wrapper.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
