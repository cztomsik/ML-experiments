{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 899.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "block_size = 64\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - 500))\n",
    "test = data.Subset(dataset, range(len(dataset) - 500, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | MinGPT | 1.4 M \n",
      "---------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.489     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 39.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now -k\n",
      "B&Y,upRo&tjRQzvYFk\n",
      "R.\n",
      "AEVpAz'KCi3vLQqKCrVUDya.VGgAl yIhduObgH\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 111/111 [00:13<00:00,  7.98it/s, loss=2.34, v_num=115]And now the ar tho halal ad,\n",
      "I oneende aves methampor athas if amee shed\n",
      "Epoch 1: 100%|██████████| 111/111 [00:14<00:00,  7.88it/s, loss=2.04, v_num=115, test_loss=2.300]And now to thim.\n",
      "\n",
      "SICHAM:\n",
      "Sidll my he sill true dowad mondsportitoves.\n",
      "\n",
      "\n",
      "Epoch 2: 100%|██████████| 111/111 [00:13<00:00,  8.18it/s, loss=1.86, v_num=115, test_loss=2.080]And now mert shall such with ard\n",
      "to has is shron is wis in sto allawaite\n",
      "Epoch 3: 100%|██████████| 111/111 [00:13<00:00,  8.27it/s, loss=1.75, v_num=115, test_loss=1.930]And now law his try should.\n",
      "\n",
      "Pery:\n",
      "All befort, why sword'd.\n",
      "My sow'd and\n",
      "Epoch 4: 100%|██████████| 111/111 [00:13<00:00,  8.21it/s, loss=1.69, v_num=115, test_loss=1.830]And now and that a movery friends her in in him.\n",
      "\n",
      "TRATIO:\n",
      "Situs mare:\n",
      "O,\n",
      "Epoch 5: 100%|██████████| 111/111 [00:13<00:00,  8.10it/s, loss=1.63, v_num=115, test_loss=1.690]And now letter son,\n",
      "The wind her have news, as I wakes him, and\n",
      "I will t\n",
      "Epoch 6: 100%|██████████| 111/111 [00:13<00:00,  8.37it/s, loss=1.6, v_num=115, test_loss=1.650] And now twick a bosounders.\n",
      "\n",
      "CLAUDIIO:\n",
      "What's it this mark.\n",
      "\n",
      "Sech:\n",
      "Will \n",
      "Epoch 7: 100%|██████████| 111/111 [00:13<00:00,  8.37it/s, loss=1.58, v_num=115, test_loss=1.640]And now is nor musine, they so bedier this death?\n",
      "\n",
      "STAGONUE:\n",
      "Now this fr\n",
      "Epoch 8: 100%|██████████| 111/111 [00:13<00:00,  8.36it/s, loss=1.56, v_num=115, test_loss=1.560]And now to the gold to the faulter serve silves.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Whom ar\n",
      "Epoch 9: 100%|██████████| 111/111 [00:13<00:00,  8.39it/s, loss=1.52, v_num=115, test_loss=1.550]And now we are stong, thy her the war, him bears:\n",
      "And sits with is we co\n",
      "Epoch 10: 100%|██████████| 111/111 [00:13<00:00,  8.42it/s, loss=1.51, v_num=115, test_loss=1.560]And now thy man the fight mistant flattingers words;\n",
      "Making who thou art\n",
      "Epoch 11: 100%|██████████| 111/111 [00:13<00:00,  8.29it/s, loss=1.51, v_num=115, test_loss=1.590]And now to his of the fair furtholy blood\n",
      "Subbistation, troying supple o\n",
      "Epoch 12: 100%|██████████| 111/111 [00:13<00:00,  7.97it/s, loss=1.48, v_num=115, test_loss=1.530]And now hat a point there soldier.\n",
      "\n",
      "RICHARDARD:\n",
      "My wimannable sorthiever\n",
      "Epoch 13: 100%|██████████| 111/111 [00:14<00:00,  7.68it/s, loss=1.47, v_num=115, test_loss=1.540]And now out, my lord:\n",
      "Ay, by, to dispite it.\n",
      "\n",
      "LORDERS:\n",
      "Ay, to-morry, I t\n",
      "Epoch 14: 100%|██████████| 111/111 [00:15<00:00,  7.30it/s, loss=1.47, v_num=115, test_loss=1.460]And now and to thy fair to at time tortune to they descate\n",
      "And wrath min\n",
      "Epoch 15: 100%|██████████| 111/111 [00:15<00:00,  7.15it/s, loss=1.46, v_num=115, test_loss=1.500]And now sigh subject to me, he\n",
      "hath he may hastill bed, and not wheart u\n",
      "Epoch 16: 100%|██████████| 111/111 [00:15<00:00,  7.14it/s, loss=1.45, v_num=115, test_loss=1.460]And now my femelict, as teal, thy bring is\n",
      "By made melt'd he wife, or ou\n",
      "Epoch 17: 100%|██████████| 111/111 [00:15<00:00,  7.18it/s, loss=1.44, v_num=115, test_loss=1.460]And now in my hotherself\n",
      "We mean warm, so, for I have shopion of thee:\n",
      "H\n",
      "Epoch 18: 100%|██████████| 111/111 [00:15<00:00,  7.26it/s, loss=1.44, v_num=115, test_loss=1.450]And now my both; all agains his duke of a daughter.\n",
      "He to accusion his? \n",
      "Epoch 19: 100%|██████████| 111/111 [00:15<00:00,  7.33it/s, loss=1.44, v_num=115, test_loss=1.470]And now I do, where you do not to the capes,\n",
      "Is the provided by sield hi\n",
      "Epoch 20: 100%|██████████| 111/111 [00:15<00:00,  7.35it/s, loss=1.41, v_num=115, test_loss=1.450]And now by hell.\n",
      "\n",
      "Lord:\n",
      "And so, now I do not allow me beg, this wrunk.\n",
      "\n",
      "\n",
      "Epoch 21: 100%|██████████| 111/111 [00:14<00:00,  7.42it/s, loss=1.42, v_num=115, test_loss=1.410]And now through sear.\n",
      "\n",
      "Citizens:\n",
      "Now made the face that as a proof that \n",
      "Epoch 22: 100%|██████████| 111/111 [00:14<00:00,  7.45it/s, loss=1.4, v_num=115, test_loss=1.470] And now my crown. Clifford, here entreats offer\n",
      "From blood in scent of t\n",
      "Epoch 23: 100%|██████████| 111/111 [00:14<00:00,  7.46it/s, loss=1.41, v_num=115, test_loss=1.450]And now must mooneys: supper to there withal\n",
      "In safe it he will drayeres\n",
      "Epoch 24: 100%|██████████| 111/111 [00:14<00:00,  7.50it/s, loss=1.39, v_num=115, test_loss=1.420]And now mercy that high hour\n",
      "That stabb'd heaven and how shall, at our b\n",
      "Epoch 24: 100%|██████████| 111/111 [00:14<00:00,  7.44it/s, loss=1.39, v_num=115, test_loss=1.400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 111/111 [00:14<00:00,  7.43it/s, loss=1.39, v_num=115, test_loss=1.400]\n"
     ]
    }
   ],
   "source": [
    "# inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "class MinGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[DecoderLayer(embed_dim, dropout) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(input_ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            input_ids = torch.cat((input_ids, step_res), dim=1)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = AFTLocal(embed_dim, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size)).to(bool).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn(x, self.mask[:T, :T])\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class AFTLocal(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.wbias = nn.Parameter(torch.Tensor(block_size, block_size), requires_grad=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        nn.init.constant_(self.wbias, 0.5)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        wbias = self.wbias[:T,:T].masked_fill(mask[:T, :T], -float(\"Inf\"))\n",
    "        temp = wbias.exp() @ (k.exp() * v)\n",
    "        temp2 = temp / (wbias.exp() @ k.exp())\n",
    "        Yt = F.sigmoid(q) * temp2\n",
    "\n",
    "        return self.proj(Yt)\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.004):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = MinGPT(vocab_size, embed_dim=192, num_layers=3, dropout=0.1)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=64, num_workers=0, sampler=data.RandomSampler(train, False, 5_000))\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=16, num_workers=0)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor([dataset.stoi[s] for s in \"And now \"], dtype=torch.long)[None,...].to(device)\n",
    "            y = self.model.generate(x, 64)[0]\n",
    "            print(\"\".join([dataset.itos[int(i)] for i in y]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else \"cpu\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! then Werry hath and my foe.\n",
      "Thou they dangers, well to be sent you.\n",
      "\n",
      "SICINIUS:\n",
      "He shall be pirge;\n",
      "How seven thing I hear at feators' discord?\n",
      "\n",
      "MENENIUS:\n",
      "And wrongs from the states and breathed have and moon!\n",
      "\n",
      "Clown:\n",
      "I think for a cuption and for his light,\n",
      "Where herears the great off thy strength.\n",
      "Ay, boy! which, then most are slay mother fair\n",
      "With touch'd and as out and, be sugar, son thee\n",
      "That too silk thing but that yet mark,\n",
      "But what I have drunk as harm.\n",
      "\n",
      "CAMILLO:\n",
      "On the light wall. The which is the head was fight\n",
      "To the person to far all, but thing me\n",
      "Than you man be step'd my father of a won;\n",
      "He ave follow and my fearful forefiends\n",
      "A \n"
     ]
    }
   ],
   "source": [
    "y = model.to(device).model.generate(torch.tensor([dataset.stoi[s] for s in \"O God, O God!\"]).unsqueeze(0).to(device), 650)\n",
    "print(\"\".join([dataset.itos[int(i)] for i in y[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c070a6546767260d3817b5bdd38c64a6478cec33d40e8bb7d9bbc980115d8646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
