{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 707.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "block_size = 64\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - 500))\n",
    "test = data.Subset(dataset, range(len(dataset) - 500, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | MinGPT | 1.4 M \n",
      "---------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.489     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 60.88it/s]And now -k\n",
      "B&Y,upRo&tjRQzvYFk\n",
      "R.\n",
      "AEVpAz'KCi3vLQqKCrVUDya.VGgAl yIhduObgH\n",
      "Epoch 0: 100%|██████████| 111/111 [00:14<00:00,  7.68it/s, loss=2.34, v_num=83]And now the\n",
      "thowe t omys od as.\n",
      "LET: hor best iset ldn lot,s til ard an \n",
      "Epoch 1: 100%|██████████| 111/111 [00:14<00:00,  7.72it/s, loss=2.07, v_num=83, test_loss=2.490]And now hering the,\n",
      "I a sher mer florth frinel drrarthe thichs.\n",
      "\n",
      "LES Bet\n",
      "Epoch 2: 100%|██████████| 111/111 [00:14<00:00,  7.91it/s, loss=1.9, v_num=83, test_loss=2.150] And now it of theer.\n",
      "How sway, my stay,\n",
      "Well, and thorlead with hime twi\n",
      "Epoch 3: 100%|██████████| 111/111 [00:13<00:00,  7.96it/s, loss=1.8, v_num=83, test_loss=1.970]And now me; and their where bling hurdst all.\n",
      "Add theat and with a geond\n",
      "Epoch 4: 100%|██████████| 111/111 [00:13<00:00,  8.24it/s, loss=1.72, v_num=83, test_loss=1.870]And now felly's farth, I have warwell sove.\n",
      "\n",
      "AUTOLYCUS:\n",
      "What I will hone\n",
      "Epoch 5: 100%|██████████| 111/111 [00:13<00:00,  8.34it/s, loss=1.67, v_num=83, test_loss=1.790]And now there, sir, flow a well:\n",
      "Thy she'll my stay. Then at a gualter a\n",
      "Epoch 6: 100%|██████████| 111/111 [00:12<00:00,  8.58it/s, loss=1.63, v_num=83, test_loss=1.710]And now man? there true man.\n",
      "\n",
      "Prozen:\n",
      "I wish such yours.\n",
      "\n",
      "MARCIUS:\n",
      "And t\n",
      "Epoch 7: 100%|██████████| 111/111 [00:13<00:00,  8.31it/s, loss=1.61, v_num=83, test_loss=1.710]And now plona the swors, I primot o' pitranies,\n",
      "Woo, thou art hy heare o\n",
      "Epoch 8: 100%|██████████| 111/111 [00:13<00:00,  8.02it/s, loss=1.58, v_num=83, test_loss=1.620]And now well.\n",
      "Misiciant he sing take a shame a marrive with a weep the s\n",
      "Epoch 9: 100%|██████████| 111/111 [00:14<00:00,  7.80it/s, loss=1.57, v_num=83, test_loss=1.610]And now so so mother! O thy sand that needs?\n",
      "While must not stomen by yo\n",
      "Epoch 10: 100%|██████████| 111/111 [00:13<00:00,  8.09it/s, loss=1.55, v_num=83, test_loss=1.600]And now you curious havious, that a page of was thought you have,\n",
      "Inted \n",
      "Epoch 11: 100%|██████████| 111/111 [00:13<00:00,  8.14it/s, loss=1.54, v_num=83, test_loss=1.540]And now will he cown the true world,\n",
      "Of you, this brother doubt; or that\n",
      "Epoch 12: 100%|██████████| 111/111 [00:13<00:00,  7.98it/s, loss=1.51, v_num=83, test_loss=1.550]And now the thou see, the can hast moon,\n",
      "That stourn abself hither of th\n",
      "Epoch 13: 100%|██████████| 111/111 [00:13<00:00,  8.11it/s, loss=1.51, v_num=83, test_loss=1.560]And now were his any son he was no dest,\n",
      "To beholy looks summer of of th\n",
      "Epoch 14: 100%|██████████| 111/111 [00:14<00:00,  7.88it/s, loss=1.49, v_num=83, test_loss=1.560]And now you do it. Come, here,\n",
      "By I am you should such a move me whils s\n",
      "Epoch 15: 100%|██████████| 111/111 [00:14<00:00,  7.57it/s, loss=1.48, v_num=83, test_loss=1.490]And now sonse and had dance, that must beseech,\n",
      "And my sovereign'd and t\n",
      "Epoch 16: 100%|██████████| 111/111 [00:15<00:00,  7.29it/s, loss=1.47, v_num=83, test_loss=1.480]And now men the provost to his mind.\n",
      "\n",
      "CAPULET:\n",
      "I have so find up, backly\n",
      "Epoch 17: 100%|██████████| 111/111 [00:15<00:00,  7.11it/s, loss=1.46, v_num=83, test_loss=1.500]And now thou sun again sudden of my bind to him:\n",
      "Of myeath wildly woman'\n",
      "Epoch 18: 100%|██████████| 111/111 [00:15<00:00,  7.04it/s, loss=1.47, v_num=83, test_loss=1.510]And now the mast bence made then anotion's footh death.\n",
      "Where much say t\n",
      "Epoch 19: 100%|██████████| 111/111 [00:15<00:00,  7.07it/s, loss=1.46, v_num=83, test_loss=1.490]And now the cheers aland your man\n",
      "To make your man housand that halfs; a\n",
      "Epoch 20: 100%|██████████| 111/111 [00:15<00:00,  7.15it/s, loss=1.45, v_num=83, test_loss=1.470]And now taken, fruit, I was a brought\n",
      "Take thee slatts at thee my crown \n",
      "Epoch 21: 100%|██████████| 111/111 [00:15<00:00,  7.17it/s, loss=1.44, v_num=83, test_loss=1.440]And now we do that will honour from the field\n",
      "So will put the palace fro\n",
      "Epoch 22: 100%|██████████| 111/111 [00:15<00:00,  7.24it/s, loss=1.43, v_num=83, test_loss=1.450]And now so man\n",
      "At on that be her body to-morrow fall more be sheep,\n",
      "Her \n",
      "Epoch 23: 100%|██████████| 111/111 [00:15<00:00,  7.25it/s, loss=1.43, v_num=83, test_loss=1.420]And now sight of that that has the weds for heaven\n",
      "As brother's foon, if\n",
      "Epoch 24: 100%|██████████| 111/111 [00:15<00:00,  7.28it/s, loss=1.42, v_num=83, test_loss=1.440]And now more when I stoops to me.\n",
      "\n",
      "ROMEO:\n",
      "An the promise, a body so fear\n",
      "Epoch 24: 100%|██████████| 111/111 [00:15<00:00,  7.21it/s, loss=1.42, v_num=83, test_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 111/111 [00:15<00:00,  7.20it/s, loss=1.42, v_num=83, test_loss=1.410]\n"
     ]
    }
   ],
   "source": [
    "# inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "class MinGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[DecoderLayer(embed_dim, dropout) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(input_ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            input_ids = torch.cat((input_ids, step_res), dim=1)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = AFTFull(embed_dim, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size)).to(bool).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn(x, self.mask[:T, :T])\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# https://arxiv.org/pdf/2105.14103.pdf\n",
    "# I am not 100% sure this is correct but it seems to work :shrug:\n",
    "class AFTFull(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.wbias = nn.Parameter(torch.Tensor(block_size, block_size), requires_grad=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        nn.init.constant_(self.wbias, 0.5)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        exp_w = self.wbias[:T,:T].masked_fill(mask[:T, :T], -float(\"Inf\")).exp()\n",
    "        exp_k = k.exp()\n",
    "\n",
    "        weighted_avg = torch.einsum(\"ik,bkj->bij\", exp_w, exp_k * v) / torch.einsum(\"ik,bkj->bij\", exp_w, exp_k)\n",
    "        Yt = F.sigmoid(q) * weighted_avg\n",
    "\n",
    "        return self.drop(self.proj(Yt))\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.004):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = MinGPT(vocab_size, embed_dim=192, num_layers=3, dropout=0.1)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=64, num_workers=0, sampler=data.RandomSampler(train, False, 5_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=16, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor([dataset.stoi[s] for s in \"And now \"], dtype=torch.long)[None,...].to(device)\n",
    "            y = self.model.generate(x, 64)[0]\n",
    "            print(\"\".join([dataset.itos[int(i)] for i in y]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else \"cpu\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "\n",
      "DORSET:\n",
      "Why, that, all so? I have\n",
      "May a harmy woeful throw storm an amazers,\n",
      "Sadden an heavens oldy suns, hath his countend\n",
      "But so two says he told with oppost how\n",
      "Of, so weep the city, with her before, to his shall he.\n",
      "Who, sir, to make my man, I'll swear to-morrow;\n",
      "And to him one home thee, sirs ifs, spoken\n",
      "With men.\n",
      "An is sert hearts we am issued the breatts again.\n",
      "That straight thou see for as have loves.\n",
      "Whilst thou that dry fond him to thone bid the send\n",
      "That brows an our cares, what I must the curst.\n",
      "\n",
      "DUKE VINCE:\n",
      "Whip to-morrow I shall the hours had seez to dishore,\n",
      "And my maste throne the curse with drum him,\n",
      "But more, sempt our cow\n"
     ]
    }
   ],
   "source": [
    "y = model.to(device).model.generate(torch.tensor([dataset.stoi[s] for s in \"O God, O God!\"]).unsqueeze(0).to(device), 650)\n",
    "print(\"\".join([dataset.itos[int(i)] for i in y[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c070a6546767260d3817b5bdd38c64a6478cec33d40e8bb7d9bbc980115d8646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
