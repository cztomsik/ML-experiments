My personal learning space. Nothing to see here, except maybe for the links below.

## Interesting Papers

- 2017 Attention is all you need\
  https://arxiv.org/abs/1706.03762

- 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\
  Encoder-only, SOTA NLP inference at the time.\
  https://arxiv.org/abs/1810.04805

- 2019 RoBERTa: A Robustly Optimized BERT Pretraining Approach\
  Improved BERT, Masking-only, dynamic masking, bigger batches, more data\
  New SOTA\
  https://arxiv.org/abs/1907.11692

- 2019 T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\
  https://arxiv.org/abs/1910.10683

- 2019 One epoch is all you need\
  https://arxiv.org/abs/1906.06669

- 2020 GPT-3: Language Models are Few-Shot Learners\
  Large enough model is capable of predicting the likely output from few examples in its prompt without any fine-tuning\
  https://arxiv.org/abs/2005.14165

- 2021 FLAN: Finetuned Language Models Are Zero-Shot Learners\
  https://arxiv.org/abs/2109.01652
  https://github.com/google-research/FLAN/blob/main/flan/templates.py

- 2021 An Attention Free Transformer\
  Alternative Transformer design, very fast\n
  https://arxiv.org/abs/2105.14103
  https://github.com/BlinkDL/RWKV-LM

- 2022 Chinchilla\
  50B model can outperform 175B GPT-3 and 280B Gopher when trained on more data (1.4T tokens)\
  https://arxiv.org/abs/2203.15556
  https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications

