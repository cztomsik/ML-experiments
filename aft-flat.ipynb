{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1344.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "block_size = 100\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - 500))\n",
    "test = data.Subset(dataset, range(len(dataset) - 500, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | MinGPT | 612 K \n",
      "---------------------------------\n",
      "612 K     Trainable params\n",
      "0         Non-trainable params\n",
      "612 K     Total params\n",
      "2.448     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 49.80it/s]And now XatxYp&U3Hybxnxwl!rwY?xMcxMn,OM:,fJoSslZdJWyUJosbxqzKO'Z$d$sb!rw\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 82/82 [00:13<00:00,  6.17it/s, loss=2.47, v_num=145]And now thir s stildeel totithe,\n",
      "Whuend ce time boul, thes at se, hal be\n",
      "Epoch 1: 100%|██████████| 82/82 [00:13<00:00,  6.11it/s, loss=2.36, v_num=145, test_loss=2.410]And now somesot death del ward\n",
      "The myou weather beay sthat hasind bre se\n",
      "Epoch 2: 100%|██████████| 82/82 [00:13<00:00,  6.03it/s, loss=2.26, v_num=145, test_loss=2.310]And now wat is and,\n",
      "Tho hal fou therer ang hethe thim sthig d,\n",
      "To an fom\n",
      "Epoch 3: 100%|██████████| 82/82 [00:13<00:00,  6.03it/s, loss=2.14, v_num=145, test_loss=2.190]And now she shon, seay and\n",
      "Whande thigh siend, ond to hatre se theres, t\n",
      "Epoch 4: 100%|██████████| 82/82 [00:13<00:00,  6.03it/s, loss=2.04, v_num=145, test_loss=2.100]And now thesee the sattlil'd on the\n",
      "the willl to be brooow ore, I be nas\n",
      "Epoch 5: 100%|██████████| 82/82 [00:13<00:00,  5.99it/s, loss=1.95, v_num=145, test_loss=2.010]And now in to macks.\n",
      "\n",
      "MENINE:\n",
      "Nor, will ya looke this ane on frome y,\n",
      "Tr\n",
      "Epoch 6: 100%|██████████| 82/82 [00:13<00:00,  6.08it/s, loss=1.88, v_num=145, test_loss=1.960]And now broth,\n",
      "And by that ingies al teesss himang to wepent shamers?\n",
      "\n",
      "S\n",
      "Epoch 7: 100%|██████████| 82/82 [00:13<00:00,  6.07it/s, loss=1.86, v_num=145, test_loss=1.890]And now way? as think ands,\n",
      "To with whelll, af their my brefen of him:\n",
      "A\n",
      "Epoch 8: 100%|██████████| 82/82 [00:13<00:00,  6.14it/s, loss=1.8, v_num=145, test_loss=1.930] And now shal tremble of away;\n",
      "Well so from madents, and sorrnges, wife.\n",
      "\n",
      "Epoch 9: 100%|██████████| 82/82 [00:13<00:00,  6.17it/s, loss=1.76, v_num=145, test_loss=1.790]And now and thou dise to to sence beatth;\n",
      "Which his sall in at all ther \n",
      "Epoch 10: 100%|██████████| 82/82 [00:13<00:00,  6.16it/s, loss=1.72, v_num=145, test_loss=1.740]And now a a lov me forfom\n",
      "Warwithok the brack take as away, at he do the\n",
      "Epoch 11: 100%|██████████| 82/82 [00:13<00:00,  6.18it/s, loss=1.7, v_num=145, test_loss=1.680] And now hath before\n",
      "For to thuse one too the curpe a would to.\n",
      "\n",
      "MENENENE\n",
      "Epoch 12: 100%|██████████| 82/82 [00:13<00:00,  6.17it/s, loss=1.69, v_num=145, test_loss=1.710]And now shiphe it, heart my purnides,\n",
      "I am besten to shroublder, and mai\n",
      "Epoch 13: 100%|██████████| 82/82 [00:13<00:00,  6.16it/s, loss=1.67, v_num=145, test_loss=1.660]And now as in sin, to throw beetch;\n",
      "But he fare of from the freselvicio,\n",
      "Epoch 14: 100%|██████████| 82/82 [00:13<00:00,  6.04it/s, loss=1.65, v_num=145, test_loss=1.630]And now to shall sendidess to such me with deesent their\n",
      "The chousest be\n",
      "Epoch 15: 100%|██████████| 82/82 [00:14<00:00,  5.79it/s, loss=1.64, v_num=145, test_loss=1.610]And now seal the to should batting one\n",
      "With he coman being in the wertch\n",
      "Epoch 16: 100%|██████████| 82/82 [00:14<00:00,  5.53it/s, loss=1.62, v_num=145, test_loss=1.600]And now be stoon.\n",
      "\n",
      "BLEOW:\n",
      "Away!\n",
      "\n",
      "ROMEO:\n",
      "What, be miser.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Epoch 17: 100%|██████████| 82/82 [00:15<00:00,  5.36it/s, loss=1.61, v_num=145, test_loss=1.600]And now swaters:\n",
      "The delte thou surse of thy heis\n",
      "To tene my times? Thav\n",
      "Epoch 18: 100%|██████████| 82/82 [00:15<00:00,  5.33it/s, loss=1.61, v_num=145, test_loss=1.560]And now this beter.\n",
      "\n",
      "LEONTES:\n",
      "Hence me at when, if I have been if me.\n",
      "\n",
      "F\n",
      "Epoch 19: 100%|██████████| 82/82 [00:15<00:00,  5.34it/s, loss=1.6, v_num=145, test_loss=1.570] And now it.\n",
      "\n",
      "First Musian:\n",
      "And all thou will to speak and the so sick,\n",
      "A\n",
      "Epoch 20: 100%|██████████| 82/82 [00:15<00:00,  5.38it/s, loss=1.59, v_num=145, test_loss=1.560]And now was, that when them is sleep they losss;\n",
      "By lesseing for a strov\n",
      "Epoch 21: 100%|██████████| 82/82 [00:15<00:00,  5.41it/s, loss=1.58, v_num=145, test_loss=1.550]And now in his the seelves\n",
      "With thricty arther of wanter must his hand;\n",
      "\n",
      "Epoch 22: 100%|██████████| 82/82 [00:14<00:00,  5.47it/s, loss=1.57, v_num=145, test_loss=1.550]And now there his hall be the should\n",
      "So stook fan that I am; indilers'd \n",
      "Epoch 23: 100%|██████████| 82/82 [00:14<00:00,  5.47it/s, loss=1.57, v_num=145, test_loss=1.500]And now to her wife, and me, but they act?\n",
      "\n",
      "HORTENSIO:\n",
      "Says they live, w\n",
      "Epoch 24: 100%|██████████| 82/82 [00:14<00:00,  5.51it/s, loss=1.56, v_num=145, test_loss=1.520]And now no blood, the mony more actinus\n",
      "To hade senting brother a courtt\n",
      "Epoch 24: 100%|██████████| 82/82 [00:14<00:00,  5.48it/s, loss=1.56, v_num=145, test_loss=1.550]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 82/82 [00:14<00:00,  5.47it/s, loss=1.56, v_num=145, test_loss=1.550]\n"
     ]
    }
   ],
   "source": [
    "# inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "class MinGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[DecoderLayer(embed_dim, dropout) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(input_ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            input_ids = torch.cat((input_ids, step_res), dim=1)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = AFTFlat(embed_dim, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn(x, self.mask[:T, :T])\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# https://arxiv.org/pdf/2105.14103.pdf\n",
    "# but w is flat vector, it's not exponentiated and we divide by\n",
    "# cumulative sum of exp(k)\n",
    "class AFTFlat(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.w = nn.Parameter(torch.ones(block_size), requires_grad=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        mask = mask[:T, :T]\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        exp_k = k.exp()\n",
    "        w = self.w[:T]\n",
    "\n",
    "        wkv = torch.einsum(\"tk,t,bkc->btc\", mask, w, exp_k * v)\n",
    "        Yt = F.sigmoid(q) * wkv / torch.cumsum(exp_k, dim=1) # better than torch.einsum(\"tk,t,bkc->btc\", mask, w, exp_k)\n",
    "        return self.drop(self.proj(Yt))\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.004):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = MinGPT(vocab_size, embed_dim=128, num_layers=3, dropout=0.1)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=100, num_workers=0, sampler=data.RandomSampler(train, False, 5_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=16, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor([dataset.stoi[s] for s in \"And now \"], dtype=torch.long)[None,...].to(device)\n",
    "            y = self.model.generate(x, 64)[0]\n",
    "            print(\"\".join([dataset.itos[int(i)] for i in y]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else \"cpu\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! amn, being hopes,\n",
      "My give livices: me, then have had beg one mark and\n",
      "For the fear seeing my bade mockeing\n",
      "And longe toouch sproken me as to tending most be sate,\n",
      "That me seen his soul, the be currnancle him.\n",
      "\n",
      "POLIXENES:\n",
      "Who betre in the cleare; I'lll far a thake me,\n",
      "Bernemined steenged faith, when be and he in our dead!\n",
      "\n",
      "KING HENRY VI:\n",
      "Hear stay at try heaven.\n",
      "\n",
      "CLUCIO:\n",
      "My pride yes weded.\n",
      "\n",
      "DUKE OF YORK:\n",
      "What we mon! fie, and your pray false off a blord,\n",
      "Mand fear is service, is in monsely cousiabh'd!\n",
      "It to do multies it. Hid the his belding seeks;\n",
      "And her sproke to mough' wife, but fly hold;\n",
      "And so bow, and best writnes; my lord,\n",
      "To how tra\n"
     ]
    }
   ],
   "source": [
    "y = model.to(device).model.generate(torch.tensor([dataset.stoi[s] for s in \"O God, O God!\"]).unsqueeze(0).to(device), 650)\n",
    "print(\"\".join([dataset.itos[int(i)] for i in y[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c070a6546767260d3817b5bdd38c64a6478cec33d40e8bb7d9bbc980115d8646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
