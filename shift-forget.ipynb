{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1112.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from shared import corpus, tokenizers, datasets\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "train, test = datasets.causal(tokenizer.encode(text).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 207 K \n",
      "-------------------------------\n",
      "207 K     Trainable params\n",
      "0         Non-trainable params\n",
      "207 K     Total params\n",
      "0.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 26.18it/s]And nowHHejiRNb$tei.nNBLcEFwvHWWtePe3H!Tb$Ap$e''U.ZewvQ.ntLjXH:JExtN WQ\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 209/209 [00:07<00:00, 29.81it/s, loss=1.6, v_num=58] And now is to be a loud benemans in to with a lifs an as if your more\n",
      "L\n",
      "Epoch 1: 100%|██████████| 209/209 [00:06<00:00, 32.11it/s, loss=1.47, v_num=58, test_loss=1.770]And now. God shame of one traitor! that's thus, sir, take the wapp'd yo\n",
      "Epoch 2: 100%|██████████| 209/209 [00:06<00:00, 32.29it/s, loss=1.4, v_num=58, test_loss=1.580] And now'st one, beatheo's honour's love ward of the words and thanks wh\n",
      "Epoch 3: 100%|██████████| 209/209 [00:06<00:00, 31.59it/s, loss=1.38, v_num=58, test_loss=1.510]And now how true son, blist thy chide he be pursuit oft on the father b\n",
      "Epoch 4: 100%|██████████| 209/209 [00:06<00:00, 32.16it/s, loss=1.35, v_num=58, test_loss=1.480]And now, good thrischange and he that those way against three for one i\n",
      "Epoch 5: 100%|██████████| 209/209 [00:06<00:00, 32.20it/s, loss=1.34, v_num=58, test_loss=1.440]And now yet bears you to my wife that myself:\n",
      "My county, would thou had\n",
      "Epoch 6: 100%|██████████| 209/209 [00:06<00:00, 31.98it/s, loss=1.32, v_num=58, test_loss=1.450]And now on.\n",
      "I know wherein to her manner.\n",
      "\n",
      "GREMIO:\n",
      "I will thrub: as tho\n",
      "Epoch 7: 100%|██████████| 209/209 [00:06<00:00, 31.49it/s, loss=1.31, v_num=58, test_loss=1.440]And now: the storm. Come, when he shall hear of your lady of our holy c\n",
      "Epoch 8: 100%|██████████| 209/209 [00:06<00:00, 31.20it/s, loss=1.31, v_num=58, test_loss=1.450]And now I supared boding,\n",
      "Southorier show his grace is no gentleman in \n",
      "Epoch 9: 100%|██████████| 209/209 [00:06<00:00, 30.89it/s, loss=1.29, v_num=58, test_loss=1.450]And now springs\n",
      "Shere as we have borne hath silence\n",
      "To murder traitor o\n",
      "Epoch 10: 100%|██████████| 209/209 [00:07<00:00, 27.74it/s, loss=1.28, v_num=58, test_loss=1.420]And now we make it desire of done,\n",
      "That is service aside,\n",
      "And thought o\n",
      "Epoch 11: 100%|██████████| 209/209 [00:08<00:00, 24.40it/s, loss=1.27, v_num=58, test_loss=1.420]And now they not; that's a merry me.\n",
      "\n",
      "PERDITA:\n",
      "I must so: whence they h\n",
      "Epoch 12: 100%|██████████| 209/209 [00:10<00:00, 19.59it/s, loss=1.27, v_num=58, test_loss=1.400]And now the friend of me, I drew all that, whereto take him send more t\n",
      "Epoch 13: 100%|██████████| 209/209 [00:14<00:00, 14.68it/s, loss=1.26, v_num=58, test_loss=1.410]And now they so. Come on.\n",
      "\n",
      "Messenger:\n",
      "He, seeing sumerce: and much be i\n",
      "Epoch 14: 100%|██████████| 209/209 [00:15<00:00, 13.74it/s, loss=1.25, v_num=58, test_loss=1.390]And now,\n",
      "Them he shall be\n",
      "The throne,\n",
      "Were be done. Our face, the best \n",
      "Epoch 15: 100%|██████████| 209/209 [00:14<00:00, 14.31it/s, loss=1.25, v_num=58, test_loss=1.410]And now neither, and sold, and there's this; moves,\n",
      "The state\n",
      "Thought o\n",
      "Epoch 16: 100%|██████████| 209/209 [00:13<00:00, 15.72it/s, loss=1.24, v_num=58, test_loss=1.420]And now, I did, if the gentleman:\n",
      "Ay; which we.\n",
      "The course:\n",
      "Why, full o\n",
      "Epoch 17: 100%|██████████| 209/209 [00:12<00:00, 17.16it/s, loss=1.23, v_num=58, test_loss=1.400]And now with me or never fate?\n",
      "That she betwixt then we stand more he m\n",
      "Epoch 18: 100%|██████████| 209/209 [00:11<00:00, 17.58it/s, loss=1.23, v_num=58, test_loss=1.410]And now the sea man all serve and marriage, thou wilt be running of tha\n",
      "Epoch 19: 100%|██████████| 209/209 [00:12<00:00, 17.22it/s, loss=1.22, v_num=58, test_loss=1.410]And now to scarce man the cares his spark? I would breat,\n",
      "In so say if \n",
      "Epoch 20: 100%|██████████| 209/209 [00:11<00:00, 18.36it/s, loss=1.22, v_num=58, test_loss=1.420]And now, methinks I, it hath'd blood,\n",
      "That be not twenty, that I was ar\n",
      "Epoch 21: 100%|██████████| 209/209 [00:10<00:00, 20.07it/s, loss=1.22, v_num=58, test_loss=1.400]And now the physic both me that tredit my sigh, that she be a loved to \n",
      "Epoch 22: 100%|██████████| 209/209 [00:10<00:00, 20.05it/s, loss=1.21, v_num=58, test_loss=1.410]And now the day.\n",
      "\n",
      "CLARENCE:\n",
      "Believe a serious little, hearing of those \n",
      "Epoch 23: 100%|██████████| 209/209 [00:10<00:00, 20.63it/s, loss=1.2, v_num=58, test_loss=1.390] And now my holy floods are not off, therefore I\n",
      "The heavens in the crow\n",
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 20.70it/s, loss=1.2, v_num=58, test_loss=1.380] And now any thing.\n",
      "\n",
      "BRUTUS:\n",
      "Where is no heudies and promised my country\n",
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 19.13it/s, loss=1.2, v_num=58, test_loss=1.400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 19.05it/s, loss=1.2, v_num=58, test_loss=1.400]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.f = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        prev = F.pad(xn, (0, 0, 1, -1))\n",
    "        prev2 = F.pad(xn, (0, 0, 2, -2))\n",
    "        f = torch.sigmoid(self.f(prev)) # prev can say what should be forgotten from prev2 (x-2)\n",
    "        q = torch.sigmoid(self.q(xn)) # what should be accepted from prev\n",
    "        v = self.v(prev) # what the prev is providing\n",
    "        attn = self.proj((q * v) - (f * self.v(prev2)))\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "import lightning as pl\n",
    "import torch.utils.data as data\n",
    "device = \"mps\"\n",
    "batch_size = 36\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor(tokenizer.encode(str).ids, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #out = self(ids[:, -block_size:])\n",
    "            out = self(ids)\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = Model(tokenizer.get_vocab_size())\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "\n",
      "First Senator:\n",
      "If you like.\n",
      "\n",
      "First Hunger than your family! mark the worst of dark;\n",
      "I came infrink, madam, farewell.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "My dearest of his people to his heart the time?\n",
      "What still, I have such perfection, the most breathe forest bid him so breathize some hurt as none miles of damned at the harms and famous and play.\n",
      "A begging of ill.\n",
      "\n",
      "MARCIUS:\n",
      "'Tyition:\n",
      "Truly son\n",
      "He had shed and down, and not so, because\n",
      "He does arriving my princely good friends against the glorious prince you have not stir seem into\n",
      "Supply she shall show your gentleman:\n",
      "It more perfect the princely good as his face?\n",
      "\n",
      "First Murderer:\n",
      "I do proved him, with an o\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
