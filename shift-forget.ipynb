{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1197.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from shared import corpus, tokenizers, datasets\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "train, test = datasets.causal(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 207 K \n",
      "-------------------------------\n",
      "207 K     Trainable params\n",
      "0         Non-trainable params\n",
      "207 K     Total params\n",
      "0.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 27.29it/s]And nowo!UTR3TRNdq;HYTWtzHohdfcEAo'IfG\n",
      "wo!!L;PPvPsOEqGAdkeldqD3GcULbgTR\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 209/209 [00:06<00:00, 30.55it/s, loss=1.6, v_num=44] And now we leave help,\n",
      "And my turn'd fear to thrain mys mercy.\n",
      "\n",
      "GLORGER\n",
      "Epoch 1: 100%|██████████| 209/209 [00:06<00:00, 32.78it/s, loss=1.47, v_num=44, test_loss=1.770]And now, as many!\n",
      "\n",
      "MENENIUS:\n",
      "The peucenning.\n",
      "\n",
      "LADY ANNE:\n",
      "Menee in ourse\n",
      "Epoch 2: 100%|██████████| 209/209 [00:06<00:00, 32.58it/s, loss=1.4, v_num=44, test_loss=1.580] And now master. Go. But my corson sorrow him spinch me to be, to him.\n",
      "\n",
      "\n",
      "Epoch 3: 100%|██████████| 209/209 [00:06<00:00, 32.84it/s, loss=1.38, v_num=44, test_loss=1.510]And now is his but it wants be neighbour'd thee of the masts; and thy s\n",
      "Epoch 4: 100%|██████████| 209/209 [00:06<00:00, 32.86it/s, loss=1.35, v_num=44, test_loss=1.480]And now on this state in mine attority.\n",
      "\n",
      "BENVOLIO:\n",
      "No, the world helse,\n",
      "Epoch 5: 100%|██████████| 209/209 [00:06<00:00, 32.70it/s, loss=1.34, v_num=44, test_loss=1.440]And now, false time thy father! Ot their wit-match, with you. Tyrrel, a\n",
      "Epoch 6: 100%|██████████| 209/209 [00:06<00:00, 30.47it/s, loss=1.32, v_num=44, test_loss=1.450]And now had none. Let me, is it:\n",
      "If that with a gentleman, old tell tha\n",
      "Epoch 7: 100%|██████████| 209/209 [00:07<00:00, 27.92it/s, loss=1.31, v_num=44, test_loss=1.440]And now, sir.\n",
      "\n",
      "BUCKINGHAM:\n",
      "If thou hast thou son.\n",
      "\n",
      "First Senator:\n",
      "You m\n",
      "Epoch 8: 100%|██████████| 209/209 [00:08<00:00, 24.40it/s, loss=1.31, v_num=44, test_loss=1.450]And now to be a gentle traces at his mate.\n",
      "\n",
      "Provost:\n",
      "He hath towards yo\n",
      "Epoch 9: 100%|██████████| 209/209 [00:11<00:00, 18.83it/s, loss=1.29, v_num=44, test_loss=1.450]And now decree; one wood of his prince:\n",
      "The salterats were it well as t\n",
      "Epoch 10: 100%|██████████| 209/209 [00:16<00:00, 12.80it/s, loss=1.28, v_num=44, test_loss=1.420]And now will,\n",
      "Or'd that's here he but with years till, sir, you the fat\n",
      "Epoch 11: 100%|██████████| 209/209 [00:17<00:00, 12.29it/s, loss=1.27, v_num=44, test_loss=1.420]And now.\n",
      "\n",
      "CAMILLO:\n",
      "I am; you mistrusty boy, if I need whom our comfort,\n",
      "Epoch 12: 100%|██████████| 209/209 [00:14<00:00, 14.69it/s, loss=1.27, v_num=44, test_loss=1.400]And now to sleep.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Within a mother's death, and mortal \n",
      "Epoch 13: 100%|██████████| 209/209 [00:14<00:00, 14.76it/s, loss=1.26, v_num=44, test_loss=1.410]And now,\n",
      "Besness of mine eyes from him to thy service wantost thou shal\n",
      "Epoch 14: 100%|██████████| 209/209 [00:11<00:00, 18.22it/s, loss=1.25, v_num=44, test_loss=1.390]And now,\n",
      "Where archant of thy work!\n",
      "\n",
      "LADY GREY:\n",
      "The swords?\n",
      "\n",
      "AUTOLYCUS:\n",
      "Epoch 15: 100%|██████████| 209/209 [00:11<00:00, 18.96it/s, loss=1.25, v_num=44, test_loss=1.410]And now with my face?\n",
      "\n",
      "COMINIUS:\n",
      "In my power,\n",
      "And to do there,\n",
      "That lik\n",
      "Epoch 16: 100%|██████████| 209/209 [00:10<00:00, 19.38it/s, loss=1.24, v_num=44, test_loss=1.420]And now, beseem than he his course.\n",
      "How now! who know thee it be the co\n",
      "Epoch 17: 100%|██████████| 209/209 [00:11<00:00, 18.02it/s, loss=1.23, v_num=44, test_loss=1.400]And now with a begs of our sick; the wear-shookly been big me three fai\n",
      "Epoch 18: 100%|██████████| 209/209 [00:11<00:00, 18.74it/s, loss=1.23, v_num=44, test_loss=1.410]And now, sir, her womb'd thyself her,\n",
      "That at his sweetest mispeak seen\n",
      "Epoch 19: 100%|██████████| 209/209 [00:11<00:00, 17.65it/s, loss=1.22, v_num=44, test_loss=1.410]And now,\n",
      "Because thee, I do continued.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "O harm, they wi\n",
      "Epoch 20: 100%|██████████| 209/209 [00:11<00:00, 18.27it/s, loss=1.22, v_num=44, test_loss=1.420]And now seal'd, I knew him\n",
      "As if they\n",
      "are one that I swear'd the shadow\n",
      "Epoch 21: 100%|██████████| 209/209 [00:11<00:00, 18.88it/s, loss=1.22, v_num=44, test_loss=1.400]And now, she is not that the people may note with the city,\n",
      "It burthen \n",
      "Epoch 22: 100%|██████████| 209/209 [00:10<00:00, 19.35it/s, loss=1.21, v_num=44, test_loss=1.410]And now; as there to mine thus speak;\n",
      "But shall bestrider is distemper,\n",
      "Epoch 23: 100%|██████████| 209/209 [00:10<00:00, 19.91it/s, loss=1.2, v_num=44, test_loss=1.390] And now spoke?\n",
      "\n",
      "KING RICHARD III:\n",
      "Stablish them dead!\n",
      "\n",
      "MENENIUS:\n",
      "The mo\n",
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 20.01it/s, loss=1.2, v_num=44, test_loss=1.380] And now merry ladies; his bones. He credit of much for this treasons th\n",
      "Epoch 24: 100%|██████████| 209/209 [00:11<00:00, 18.55it/s, loss=1.2, v_num=44, test_loss=1.400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:11<00:00, 18.48it/s, loss=1.2, v_num=44, test_loss=1.400]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.f = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        prev = F.pad(xn, (0, 0, 1, -1))\n",
    "        prev2 = F.pad(xn, (0, 0, 2, -2))\n",
    "        f = torch.sigmoid(self.f(prev)) # prev can say what should be forgotten from prev2 (x-2)\n",
    "        q = torch.sigmoid(self.q(xn)) # what should be accepted from prev\n",
    "        v = self.v(prev) # what the prev is providing\n",
    "        attn = self.proj((q * v) - (f * self.v(prev2)))\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "import lightning as pl\n",
    "import torch.utils.data as data\n",
    "device = \"mps\"\n",
    "batch_size = 36\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor(tokenizer.encode(str), dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #out = self(ids[:, -block_size:])\n",
    "            out = self(ids)\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = Model(tokenizer.vocab_size)\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "\n",
      "First Senator:\n",
      "If you like.\n",
      "\n",
      "First Hunger than your family! mark the worst of dark;\n",
      "I came infrink, madam, farewell.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "My dearest of his people to his heart the time?\n",
      "What still, I have such perfection, the most breathe forest bid him so breathize some hurt as none miles of damned at the harms and famous and play.\n",
      "A begging of ill.\n",
      "\n",
      "MARCIUS:\n",
      "'Tyition:\n",
      "Truly son\n",
      "He had shed and down, and not so, because\n",
      "He does arriving my princely good friends against the glorious prince you have not stir seem into\n",
      "Supply she shall show your gentleman:\n",
      "It more perfect the princely good as his face?\n",
      "\n",
      "First Murderer:\n",
      "I do proved him, with an o\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
