{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 945.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "block_size = 100\n",
    "test_size = 1500\n",
    "batch_size = 64\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - test_size))\n",
    "test = data.Subset(dataset, range(len(dataset) - test_size, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 226 K \n",
      "-------------------------------\n",
      "226 K     Trainable params\n",
      "0         Non-trainable params\n",
      "226 K     Total params\n",
      "0.905     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]And nowSMzROiPpyw:Vp csHDEEfNvyw:VgGa-FBSM,mBNqHAnMr\n",
      ":pCc'S\n",
      "K.z3WkhyyN$\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 118/118 [00:06<00:00, 18.63it/s, loss=2.36, v_num=78]And now so hod sand bare ade my aigh sheroouse to womes,\n",
      "I he soto he a\n",
      "Epoch 1: 100%|██████████| 118/118 [00:04<00:00, 24.40it/s, loss=2.01, v_num=78, test_loss=2.280]And nowath the the that tumer a seat\n",
      "Mared frove mane hes shat the wivi\n",
      "Epoch 2: 100%|██████████| 118/118 [00:04<00:00, 24.32it/s, loss=1.79, v_num=78, test_loss=2.010]And now who the but should armser, and son,\n",
      "I lode seopanith, sincelain\n",
      "Epoch 3: 100%|██████████| 118/118 [00:04<00:00, 24.25it/s, loss=1.69, v_num=78, test_loss=1.920]And now will tends it will would\n",
      "An hles it were time broked\n",
      "Was to the\n",
      "Epoch 4: 100%|██████████| 118/118 [00:04<00:00, 24.27it/s, loss=1.62, v_num=78, test_loss=1.840]And now.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Whyself you.\n",
      "\n",
      "BROKE VINCENTIO:\n",
      "Here havide th\n",
      "Epoch 5: 100%|██████████| 118/118 [00:04<00:00, 24.24it/s, loss=1.58, v_num=78, test_loss=1.800]And now not mut of thy wisdom sust fall: thereing is tiss,\n",
      "And have mar\n",
      "Epoch 6: 100%|██████████| 118/118 [00:04<00:00, 24.24it/s, loss=1.54, v_num=78, test_loss=1.690]And now it is thee; then.\n",
      "\n",
      "COMINIUS:\n",
      "In the strue, by their to the thre\n",
      "Epoch 7: 100%|██████████| 118/118 [00:04<00:00, 24.17it/s, loss=1.52, v_num=78, test_loss=1.640]And now thouse, standing and thus anobalm and\n",
      "to succept thy tongue, th\n",
      "Epoch 8: 100%|██████████| 118/118 [00:04<00:00, 24.12it/s, loss=1.5, v_num=78, test_loss=1.650] And now the body have begrievous,\n",
      "In bast shall be might of him to do m\n",
      "Epoch 9: 100%|██████████| 118/118 [00:04<00:00, 24.12it/s, loss=1.48, v_num=78, test_loss=1.630]And now have will will an dispose.\n",
      "\n",
      "LEONTES:\n",
      "O, my brother; I will the \n",
      "Epoch 10: 100%|██████████| 118/118 [00:04<00:00, 24.13it/s, loss=1.47, v_num=78, test_loss=1.630]And nows when are. Your posplaces?\n",
      "O man;' all his to be has set will b\n",
      "Epoch 11: 100%|██████████| 118/118 [00:04<00:00, 24.05it/s, loss=1.45, v_num=78, test_loss=1.590]And now mine\n",
      "As horly any sword, as the ghrave at lattempty\n",
      "hand for th\n",
      "Epoch 12: 100%|██████████| 118/118 [00:04<00:00, 23.88it/s, loss=1.43, v_num=78, test_loss=1.590]And now in the full of?\n",
      "O guit, he inclastlanching in petual promised\n",
      "S\n",
      "Epoch 13: 100%|██████████| 118/118 [00:04<00:00, 24.69it/s, loss=1.44, v_num=78, test_loss=1.580]And now weeps;\n",
      "The soldiers of that what hear the cannot there?\n",
      "\n",
      "Shephe\n",
      "Epoch 14: 100%|██████████| 118/118 [00:04<00:00, 24.11it/s, loss=1.42, v_num=78, test_loss=1.530]And now in sincely the days.\n",
      "\n",
      "SICINIUS:\n",
      "Have I am in this toward the de\n",
      "Epoch 15: 100%|██████████| 118/118 [00:04<00:00, 23.88it/s, loss=1.41, v_num=78, test_loss=1.550]And now the\n",
      "shook of him to cannot a hope! O how ball belove.\n",
      "\n",
      "MERCUTIO\n",
      "Epoch 16: 100%|██████████| 118/118 [00:04<00:00, 24.10it/s, loss=1.4, v_num=78, test_loss=1.530] And now.\n",
      "\n",
      "LUCIO:\n",
      "Strivy the tomb'd of mine, blood, I how infold\n",
      "Within \n",
      "Epoch 17: 100%|██████████| 118/118 [00:04<00:00, 23.84it/s, loss=1.39, v_num=78, test_loss=1.520]And now\n",
      "He made never:\n",
      "By this still-fall\n",
      "With me; we may the bolded ar\n",
      "Epoch 18: 100%|██████████| 118/118 [00:04<00:00, 24.02it/s, loss=1.39, v_num=78, test_loss=1.510]And now,\n",
      "Allack to be make their before I\n",
      "And his day.\n",
      "\n",
      "Love I dare:\n",
      "No\n",
      "Epoch 19: 100%|██████████| 118/118 [00:04<00:00, 23.82it/s, loss=1.39, v_num=78, test_loss=1.510]And now to make me them outwixt,\n",
      "Were the shepherd; why, and this is da\n",
      "Epoch 20: 100%|██████████| 118/118 [00:05<00:00, 23.09it/s, loss=1.37, v_num=78, test_loss=1.490]And now, to be nanth high on the rest.\n",
      "\n",
      "FLORIZEL:\n",
      "Why dost, we put of a\n",
      "Epoch 21: 100%|██████████| 118/118 [00:05<00:00, 22.98it/s, loss=1.37, v_num=78, test_loss=1.470]And now, sure that if your he,\n",
      "And bless'd my truth her, and fine in br\n",
      "Epoch 22: 100%|██████████| 118/118 [00:05<00:00, 21.27it/s, loss=1.36, v_num=78, test_loss=1.460]And now and death.\n",
      "\n",
      "Prince:\n",
      "Not then that I say, treat, I tell teart,\n",
      "A\n",
      "Epoch 23: 100%|██████████| 118/118 [00:05<00:00, 20.71it/s, loss=1.36, v_num=78, test_loss=1.460]And now, whom those the worthy own willing and\n",
      "Where you as the tribued\n",
      "Epoch 24: 100%|██████████| 118/118 [00:05<00:00, 19.94it/s, loss=1.35, v_num=78, test_loss=1.450]And now to down,\n",
      "How to come, but awhile: for when to be\n",
      "to return and \n",
      "Epoch 24: 100%|██████████| 118/118 [00:06<00:00, 18.06it/s, loss=1.35, v_num=78, test_loss=1.480]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 118/118 [00:06<00:00, 18.00it/s, loss=1.35, v_num=78, test_loss=1.480]\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=92, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_emb = nn.Embedding(vocab_size, embed_dim),\n",
    "            pos_emb = nn.Embedding(block_size, embed_dim),\n",
    "            layers = nn.Sequential(*[Layer(embed_dim, num_heads) for _ in range(num_layers)]),\n",
    "            norm = nn.LayerNorm(embed_dim),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos = torch.arange(0, x.size(1), dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        x = self.transformer.token_emb(x) + self.transformer.pos_emb(pos)\n",
    "        x = self.transformer.norm(self.transformer.layers(x))\n",
    "        return self.lm_head(x)\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size)).to(bool).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        x = x + self.attn(xn, xn, xn, need_weights=False, attn_mask=self.mask[:T, :T])[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor([dataset.stoi[ch] for ch in str], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return \"\".join([dataset.itos[int(i)] for i in ids[0]])\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "\n",
      "PERCY:\n",
      "All, make I thank, I send to tell,\n",
      "Where yourself those from them that shall be high,\n",
      "For that to a sighto alter him; found wonder heart\n",
      "As to pain thy seasilds.\n",
      "\n",
      "KING LEWIS XI:\n",
      "Hark'd wined answer with the trusts of the thurn way\n",
      "The coming out at a\n",
      "was my peace absed,\n",
      "This, was shall be her tale him.\n",
      "\n",
      "LEONTES:\n",
      "How we say that have send to the woe of woe?\n",
      "I will begin the can our siege,\n",
      "Be now in and strengthend as\n",
      "To carvile's, sevent their answers?\n",
      "O hath hind was intend accused?\n",
      "\n",
      "MARCIUS:\n",
      "I had now!\n",
      "\n",
      "BRUTUS,\n",
      "Which in his present for two alone: it,\n",
      "And humility.\n",
      "O, sir, so my time wrongs of these come?\n",
      "\n",
      "LUCIO:\n",
      "Is no, haved she?\n",
      "\n",
      "AUTOLYCUS:\n",
      "How is forbid for soldiers;\n",
      "So men to the royal of and had,\n",
      "As must bear that this soul crown a law in too\n",
      "I mine seated.\n",
      "\n",
      "FLORIZEL:\n",
      "Too well, and your could for some fools,\n",
      "Which were we secreth and stard; this lack before you\n",
      "As young, in my before tears.' apple, and with his crease sufford\n",
      "By hand a bawd; but for for thee forth; but of soul,\n",
      "And sovereignity \n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"O God, O God!\", 1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
