{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "import math\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "shakespeare = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "block_size = 64\n",
    "\n",
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = CharDataset(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | MinGPT | 818 K \n",
      "---------------------------------\n",
      "818 K     Trainable params\n",
      "0         Non-trainable params\n",
      "818 K     Total params\n",
      "3.273     Total estimated model params size (MB)\n",
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/15685 [00:00<?, ?it/s] O God, O God!bQOoOdtYEJ:QZIjNc?$OPvxEeiktLWOgbuOLtSg!fzTL;p!t-VmTVlJumllcgXch\n",
      "Epoch 0:   6%|▋         | 1000/15685 [02:41<39:28,  6.20it/s, loss=2.12, v_num=8]O God, O God!\n",
      "\n",
      "Cillich yet hatelf poitizecthe  ples nond h his your\n",
      "Thinteld \n",
      "Epoch 0:  13%|█▎        | 2000/15685 [05:29<37:31,  6.08it/s, loss=1.85, v_num=8]O God, O God!\n",
      "Coufish the have one prisheds some. His\n",
      "How tay fore fringo: wa\n",
      "Epoch 0:  19%|█▉        | 3000/15685 [08:17<35:01,  6.04it/s, loss=1.97, v_num=8]O God, O God!\n",
      "That fall mitchame o a hand witham teis whon othen tark,\n",
      "Thouse\n",
      "Epoch 0:  26%|██▌       | 4000/15685 [11:21<33:09,  5.87it/s, loss=1.81, v_num=8]O God, O God! my gracie, here in is thought\n",
      "Thusersiivy so we to your lover i\n",
      "Epoch 0:  32%|███▏      | 5000/15685 [14:27<30:54,  5.76it/s, loss=1.87, v_num=8]O God, O God!\n",
      "My more say stry a do,\n",
      "For to to free trike the, bring farest t\n",
      "Epoch 0:  38%|███▊      | 6000/15685 [17:34<28:22,  5.69it/s, loss=1.8, v_num=8] O God, O God!\n",
      "Alag mon my wale hearns, and man of have haver mean'd woe.\n",
      "\n",
      "KIN\n",
      "Epoch 0:  45%|████▍     | 7000/15685 [20:38<25:37,  5.65it/s, loss=1.84, v_num=8]O God, O God! some your have part smore hen to froes.\n",
      "\n",
      "SALd--\n",
      "With the will-b\n",
      "Epoch 0:  51%|█████     | 8000/15685 [23:52<22:55,  5.59it/s, loss=1.77, v_num=8]O God, O God! when or agains of his ction,\n",
      "Tybethinks hale an tree his stilm\n",
      "\n",
      "Epoch 0:  57%|█████▋    | 9000/15685 [27:02<20:05,  5.55it/s, loss=1.51, v_num=8]O God, O God!\n",
      "Antimes of thour countle in this\n",
      "And face, and slaw her him som\n",
      "Epoch 0:  64%|██████▍   | 10000/15685 [29:56<17:01,  5.57it/s, loss=1.47, v_num=8]O God, O God!\n",
      "\n",
      "Still:\n",
      "See, as would I strangther warwick hall a brothen.\n",
      "\n",
      "Sec\n",
      "Epoch 0:  70%|███████   | 11000/15685 [32:50<13:59,  5.58it/s, loss=1.42, v_num=8]O God, O God!\n",
      "\n",
      "KING EDWARD IV:\n",
      "That's pratcher speak you thou, have his dispe\n",
      "Epoch 0:  77%|███████▋  | 12000/15685 [35:52<11:01,  5.57it/s, loss=1.72, v_num=8]O God, O God!\n",
      "\n",
      "ANTIGONUS:\n",
      "Mears:\n",
      "Here to\n",
      "Whom sinces she persations infive, t\n",
      "Epoch 0:  83%|████████▎ | 13000/15685 [38:50<08:01,  5.58it/s, loss=1.67, v_num=8]O God, O God!\n",
      "\n",
      "GentlemiER\n",
      "Were not that so the checillo igage of they businne\n",
      "Epoch 0:  89%|████████▉ | 14000/15685 [41:51<05:02,  5.57it/s, loss=1.68, v_num=8]O God, O God!\n",
      "But of should not as fool.\n",
      "\n",
      "ANGELO:\n",
      "Thou art not him,\n",
      "By sharp'\n",
      "Epoch 0:  96%|█████████▌| 15000/15685 [45:19<02:04,  5.52it/s, loss=1.51, v_num=8]O God, O God! Lay these hanger! If then\n",
      "than most from this swunderiary as hi\n",
      "Epoch 0: 100%|██████████| 15685/15685 [47:50<00:00,  5.46it/s, loss=1.57, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 15685/15685 [47:50<00:00,  5.46it/s, loss=1.57, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "# and https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
    "# defaults for gpt-mini\n",
    "class MinGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=192, num_heads=6, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_emb = nn.Embedding(vocab_size, embed_dim),\n",
    "            pos_emb = nn.Embedding(block_size, embed_dim),\n",
    "            drop = nn.Dropout(dropout),\n",
    "            layers = nn.Sequential(*[DecoderLayer(embed_dim, num_heads, dropout) for _ in range(num_layers)]),\n",
    "            norm = nn.LayerNorm(embed_dim),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pos = torch.arange(0, x.size(1), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        x = self.transformer.token_emb(x) + self.transformer.pos_emb(pos)\n",
    "        x = self.transformer.drop(x)\n",
    "        x = self.transformer.layers(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(input_ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            input_ids = torch.cat((input_ids, step_res), dim=1)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size)).to(bool).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn(x, x, x, need_weights=False, attn_mask=self.mask[:T, :T])[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.model = MinGPT(vocab_size, embed_dim=128, num_heads=4, num_layers=4, dropout=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "        if (batch_idx % 1000) == 0:\n",
    "            with torch.no_grad():\n",
    "                x = torch.tensor([dataset.stoi[s] for s in \"O God, O God!\"], dtype=torch.long)[None,...].to(device)\n",
    "                y = self.model.generate(x, 64)[0]\n",
    "                print(\"\".join([dataset.itos[int(i)] for i in y]))\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, max_epochs=1, enable_progress_bar=True, log_every_n_steps=100, accelerator=\"gpu\" if device == \"cuda\" else \"cpu\")\n",
    "trainer.fit(model, torch.utils.data.DataLoader(dataset, batch_size=64, num_workers=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I well, say, thou can me, so, I with his thought thy stock\n",
      "To-singest her ask. Fair hers, all this idle.\n",
      "\n",
      "PETReARINA:\n",
      "Sir, him it scares him, he, sim thou sat, sile.\n",
      "Thou, belut should you well me? Baptient more have me.\n",
      "Bapting at see not I way.\n",
      "\n",
      "TGR:\n",
      "He charge hold, wish, yell all, an these shall I may.\n",
      "\n",
      "GRAMIO:\n",
      "Nay's tressist men and by thee ye the believe\n",
      "For my mother take than that should. Any what\n",
      "all her ald, we make in all thither by to word\n",
      "So this her's father her heaven in you; with, faulto me.\n",
      "\n",
      "PATR:\n",
      "Hare when sister, you before if whom I my go,\n",
      "Which is it, for that and struck wrion well.\n",
      "\n",
      "GRAMINA:\n",
      "I beay, subjech tweay? Bianca, you that maigo? Biance, what you that so well not.\n",
      "Twhid yourning you sh, whose all when thirt and widow me.\n",
      "\n",
      "GRA:\n",
      "Belield sister, sir, thee, she your loved,\n",
      "The friend, stand held the to brive for we twenty.\n",
      "Hard a but as stand the friends, belike old.\n",
      "\n",
      "TRANIA:\n",
      "O, suitor a mostlemen, Is their this gived, you have you\n",
      "Stild all so we this sthou this so, tuth, free it thy greead?\n",
      "\n",
      "TRANIO:\n",
      "You mides yea? I beck what my will set you by be me,\n",
      "Your swond shall will that any myself or so hope his the myseluter;\n",
      "And his softers adve that me in a soundry me,\n",
      "And why, it sir, accrieve. Here have, where you feep you, as she\n",
      "born I have you and new the matters.\n",
      "\n",
      "LO:\n",
      "If I will think if you this a while I must\n",
      "The lovedly but in me, this but than me; by save.\n",
      "\n",
      "BAPTISTA:\n",
      "Signions, with her I mast ting me, it is, me bed.\n",
      "Ws evock me friend blief me.\n",
      "That my tuster me so me bech and in are my beass.\n",
      "\n",
      "TRAHIHARINA:\n",
      "How my be sign, that ther my bettey word, thou will she\n",
      "since you beloved all him thus? That you? bosome\n",
      "You she drusted it a grised. Wed to a wooks.\n",
      "\n",
      "HARTANTP:\n",
      "As well, side shre my sic, and, silell you whre you fair,\n",
      "All you not now all be be drace you dress;\n",
      "The to the will geam in, and will, must me words;\n",
      "What word me; in so wrerch I we be all.\n",
      "Should here in him me will shill, my deaw woo me.\n",
      "\n",
      "BATP:\n",
      "I shall more, see, afflcy m\n"
     ]
    }
   ],
   "source": [
    "y = model.to(device).model.generate(torch.tensor([dataset.stoi[s] for s in \"ROMEO:\"]).unsqueeze(0).to(device), 2000)\n",
    "print(\"\".join([dataset.itos[int(i)] for i in y[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c070a6546767260d3817b5bdd38c64a6478cec33d40e8bb7d9bbc980115d8646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
