{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1308.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "block_size = 256\n",
    "test_size = 1500\n",
    "batch_size = 36\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - test_size))\n",
    "test = data.Subset(dataset, range(len(dataset) - test_size, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 207 K \n",
      "-------------------------------\n",
      "207 K     Trainable params\n",
      "0         Non-trainable params\n",
      "207 K     Total params\n",
      "0.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 62.66it/s]And now;&rUl:lWDWW:v!sUDSYSd$BzAjtOZuuFU\n",
      "&YntlhV&jGyknZiLFjXiYZCoXt;pNK\n",
      "Epoch 0: 100%|██████████| 209/209 [00:07<00:00, 28.10it/s, loss=1.71, v_num=41]And now it welp.\n",
      "\n",
      "BROMEO:\n",
      "Which out; all of make was slead here and bea\n",
      "Epoch 1: 100%|██████████| 209/209 [00:07<00:00, 28.09it/s, loss=1.58, v_num=41, test_loss=1.890]And now that shrely with his ble my greaten, too shake too twice her ma\n",
      "Epoch 2: 100%|██████████| 209/209 [00:07<00:00, 28.60it/s, loss=1.53, v_num=41, test_loss=1.790]And now a priment and waller.\n",
      "\n",
      "SIR SHASTANLTYOt my lord.'\n",
      "Why, that dro\n",
      "Epoch 3: 100%|██████████| 209/209 [00:07<00:00, 28.46it/s, loss=1.49, v_num=41, test_loss=1.620]And now\n",
      "And true, honest, and thou himself. O, for himself, with throat\n",
      "Epoch 4: 100%|██████████| 209/209 [00:07<00:00, 28.47it/s, loss=1.47, v_num=41, test_loss=1.640]And now my farewell be better, he mine.\n",
      "Home time thou should dares hel\n",
      "Epoch 5: 100%|██████████| 209/209 [00:07<00:00, 28.48it/s, loss=1.45, v_num=41, test_loss=1.610]And now the serves.\n",
      "\n",
      "CLARENCE:\n",
      "There he husband this frame,\n",
      "Appress'd h\n",
      "Epoch 6: 100%|██████████| 209/209 [00:07<00:00, 28.33it/s, loss=1.44, v_num=41, test_loss=1.520]And now, and will deny;\n",
      "We see out of they have boy! the puliet, my lie\n",
      "Epoch 7: 100%|██████████| 209/209 [00:07<00:00, 28.27it/s, loss=1.42, v_num=41, test_loss=1.520]And now the people's mones\n",
      "they wench:\n",
      "I had hereof one apiness?\n",
      "The he\n",
      "Epoch 8: 100%|██████████| 209/209 [00:07<00:00, 28.23it/s, loss=1.4, v_num=41, test_loss=1.550] And now marce woman shall be throne issue.\n",
      "\n",
      "KING EDWARD II:\n",
      "I'll not an\n",
      "Epoch 9: 100%|██████████| 209/209 [00:07<00:00, 28.34it/s, loss=1.42, v_num=41, test_loss=1.490]And now,\n",
      "I have the she and fellow at meet.\n",
      "Thou hadst thrive me a mire\n",
      "Epoch 10: 100%|██████████| 209/209 [00:07<00:00, 28.38it/s, loss=1.4, v_num=41, test_loss=1.480] And now, if\n",
      "Will be perchiown is the shall, she many hear what is't the\n",
      "Epoch 11: 100%|██████████| 209/209 [00:07<00:00, 28.34it/s, loss=1.39, v_num=41, test_loss=1.520]And now is this some wounds are the gentle,\n",
      "And appear of that the pand\n",
      "Epoch 12: 100%|██████████| 209/209 [00:07<00:00, 27.97it/s, loss=1.38, v_num=41, test_loss=1.470]And now! thy secure over to this world: I\n",
      "circumstantle, brave yet you \n",
      "Epoch 13: 100%|██████████| 209/209 [00:07<00:00, 28.05it/s, loss=1.37, v_num=41, test_loss=1.470]And now, some that, I. What down with your carried other doth doth surp\n",
      "Epoch 14: 100%|██████████| 209/209 [00:07<00:00, 28.09it/s, loss=1.36, v_num=41, test_loss=1.470]And now myself?\n",
      "They are this foul all another's twenty house,\n",
      "What is \n",
      "Epoch 15: 100%|██████████| 209/209 [00:07<00:00, 28.14it/s, loss=1.37, v_num=41, test_loss=1.440]And now, dare and\n",
      "some ancient we with suns, what to the gods harsed a \n",
      "Epoch 16: 100%|██████████| 209/209 [00:07<00:00, 27.80it/s, loss=1.37, v_num=41, test_loss=1.430]And now at it did the stirrth, for the may have time!\n",
      "O could not:\n",
      "I wi\n",
      "Epoch 17: 100%|██████████| 209/209 [00:07<00:00, 28.14it/s, loss=1.36, v_num=41, test_loss=1.450]And now\n",
      "Of blood as a grands to appare no\n",
      "deed in a boot up-shake our h\n",
      "Epoch 18: 100%|██████████| 209/209 [00:07<00:00, 28.12it/s, loss=1.36, v_num=41, test_loss=1.430]And now?\n",
      "\n",
      "LEONTES:\n",
      "Nay, he time, who have no many women and she, here s\n",
      "Epoch 19: 100%|██████████| 209/209 [00:07<00:00, 28.17it/s, loss=1.35, v_num=41, test_loss=1.450]And now! they would sometimes at hath he bosom,\n",
      "Bid me object; and the \n",
      "Epoch 20: 100%|██████████| 209/209 [00:07<00:00, 28.15it/s, loss=1.34, v_num=41, test_loss=1.430]And now that them, whom, sir, take up of you to the bale, thou spected \n",
      "Epoch 21: 100%|██████████| 209/209 [00:07<00:00, 28.18it/s, loss=1.34, v_num=41, test_loss=1.430]And now\n",
      "A grace,\n",
      "Althou would he done,\n",
      "And take thee them:\n",
      "Then lies th\n",
      "Epoch 22: 100%|██████████| 209/209 [00:07<00:00, 27.41it/s, loss=1.34, v_num=41, test_loss=1.440]And now!\n",
      "\n",
      "PERDITA:\n",
      "Sirrah of our pitching by his loving hath all their \n",
      "Epoch 23: 100%|██████████| 209/209 [00:08<00:00, 25.74it/s, loss=1.32, v_num=41, test_loss=1.430]And now the heard old fellow the spect:\n",
      "I will death thou the pleasure \n",
      "Epoch 24: 100%|██████████| 209/209 [00:08<00:00, 24.49it/s, loss=1.33, v_num=41, test_loss=1.430]And now, and tell the hollow, things so breed;\n",
      "And the counter oft by s\n",
      "Epoch 24: 100%|██████████| 209/209 [00:09<00:00, 22.95it/s, loss=1.33, v_num=41, test_loss=1.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:09<00:00, 22.87it/s, loss=1.33, v_num=41, test_loss=1.420]\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        k, v = self.kv(F.pad(xn, (0, 0, 1, -1))).chunk(2, dim=-1)\n",
    "        w = torch.einsum(\"ijk,ijk->ij\", q, k) # / math.sqrt(C)\n",
    "        attn = self.proj(w[:, :, None] * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor([dataset.stoi[ch] for ch in str], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return \"\".join([dataset.itos[int(i)] for i in ids[0]])\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for b in model.model.transformer[1]:\n",
    "    plt.figure()\n",
    "    plt.imshow(b.q.weight.detach())\n",
    "    plt.figure()\n",
    "    plt.imshow(b.mlp[0].weight.detach())\n",
    "    plt.figure()\n",
    "    plt.imshow(b.mlp[2].weight.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(\"O God, O God!\".rjust(block_size), 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
