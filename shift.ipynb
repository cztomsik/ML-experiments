{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1229.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from shared import corpus, tokenizers, datasets\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "train, test = datasets.causal(tokenizer.encode(text).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 190 K \n",
      "-------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 22.64it/s]And nowX;rncvc'UBles&aRwgBobqxZgUv;Si&Rw-l&cvXqMw:ce3VcNxoiqMvtKIIkkcfE\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 209/209 [00:05<00:00, 35.82it/s, loss=1.67, v_num=59]And now to true in them at your grats think flembs\n",
      "Them thy purse me, i\n",
      "Epoch 1: 100%|██████████| 209/209 [00:05<00:00, 38.31it/s, loss=1.53, v_num=59, test_loss=1.830]And now, and his dasting ship's the hand,\n",
      "Which his shall this fight an\n",
      "Epoch 2: 100%|██████████| 209/209 [00:05<00:00, 38.26it/s, loss=1.46, v_num=59, test_loss=1.680]And now throat my side: an oble to sleep: interton burding another.\n",
      "\n",
      "DU\n",
      "Epoch 3: 100%|██████████| 209/209 [00:05<00:00, 38.33it/s, loss=1.43, v_num=59, test_loss=1.590]And now woman many welces and worthy heirs in.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The sen\n",
      "Epoch 4: 100%|██████████| 209/209 [00:05<00:00, 38.94it/s, loss=1.41, v_num=59, test_loss=1.530]And now him will persual dance thank not all that I am somethink may be\n",
      "Epoch 5: 100%|██████████| 209/209 [00:05<00:00, 38.99it/s, loss=1.39, v_num=59, test_loss=1.540]And now? which these my body byself and from him bearing me,\n",
      "Thou but I\n",
      "Epoch 6: 100%|██████████| 209/209 [00:05<00:00, 38.57it/s, loss=1.38, v_num=59, test_loss=1.510]And now young princes the plaint,\n",
      "Or shall breast have a present is den\n",
      "Epoch 7: 100%|██████████| 209/209 [00:05<00:00, 38.55it/s, loss=1.37, v_num=59, test_loss=1.490]And now:\n",
      "I did make:\n",
      "Be me.\n",
      "\n",
      "SOMERSET:\n",
      "These to and him thou call attem\n",
      "Epoch 8: 100%|██████████| 209/209 [00:05<00:00, 37.86it/s, loss=1.35, v_num=59, test_loss=1.460]And now nothily to themselves in then I'll we were shape,\n",
      "Tell thus lor\n",
      "Epoch 9: 100%|██████████| 209/209 [00:05<00:00, 37.71it/s, loss=1.35, v_num=59, test_loss=1.450]And now,\n",
      "Whole in the city thrust is live,\n",
      "Advout of a letters;\n",
      "Whom I \n",
      "Epoch 10: 100%|██████████| 209/209 [00:05<00:00, 37.50it/s, loss=1.35, v_num=59, test_loss=1.470]And now: he boy: all their provost:\n",
      "Some more he was a grusion of arriv\n",
      "Epoch 11: 100%|██████████| 209/209 [00:05<00:00, 36.97it/s, loss=1.34, v_num=59, test_loss=1.440]And now thy house the worthiesine,\n",
      "Shall the fair dreadful blue. The ch\n",
      "Epoch 12: 100%|██████████| 209/209 [00:05<00:00, 36.35it/s, loss=1.34, v_num=59, test_loss=1.460]And now far her commons of his pair of a bloody counfead who watch mist\n",
      "Epoch 13: 100%|██████████| 209/209 [00:05<00:00, 37.12it/s, loss=1.34, v_num=59, test_loss=1.450]And now I dare honour! will straight.\n",
      "It is: would death;\n",
      "And before I \n",
      "Epoch 14: 100%|██████████| 209/209 [00:05<00:00, 35.85it/s, loss=1.33, v_num=59, test_loss=1.410]And now fair shrift: and had to thyself a minute\n",
      "Have with the bosom.\n",
      "\n",
      "\n",
      "Epoch 15: 100%|██████████| 209/209 [00:05<00:00, 37.49it/s, loss=1.32, v_num=59, test_loss=1.430]And now; but not\n",
      "And white him trades,\n",
      "Which in a storms you may shall \n",
      "Epoch 16: 100%|██████████| 209/209 [00:05<00:00, 36.82it/s, loss=1.31, v_num=59, test_loss=1.420]And now fearful answer than awhile to stay will this may\n",
      "Is sudden's do\n",
      "Epoch 17: 100%|██████████| 209/209 [00:05<00:00, 36.43it/s, loss=1.31, v_num=59, test_loss=1.400]And now\n",
      "Than myself\n",
      "As angels to right against this purst nothing miss \n",
      "Epoch 18: 100%|██████████| 209/209 [00:05<00:00, 35.14it/s, loss=1.31, v_num=59, test_loss=1.420]And now at holding, and said's speak, in her, which may,\n",
      "to my cheaven\n",
      "\n",
      "Epoch 19: 100%|██████████| 209/209 [00:06<00:00, 31.84it/s, loss=1.3, v_num=59, test_loss=1.420] And now, bid, and dost the could so.\n",
      "\n",
      "LUCENTIO:\n",
      "Therefore,\n",
      "To wind\n",
      "That\n",
      "Epoch 20: 100%|██████████| 209/209 [00:07<00:00, 28.87it/s, loss=1.31, v_num=59, test_loss=1.410]And now, sir, hence:\n",
      "All be sweetle prevent.\n",
      "\n",
      "COMINIUS:\n",
      "Take thus subje\n",
      "Epoch 21: 100%|██████████| 209/209 [00:08<00:00, 25.38it/s, loss=1.3, v_num=59, test_loss=1.430] And now mad; for I have him should my life,\n",
      "And the devised, and when i\n",
      "Epoch 22: 100%|██████████| 209/209 [00:09<00:00, 23.12it/s, loss=1.29, v_num=59, test_loss=1.410]And now I pale\n",
      "I see, to thy soul?\n",
      "\n",
      "CLAUDIO:\n",
      "Thus as you, sir,\n",
      "Writing \n",
      "Epoch 23: 100%|██████████| 209/209 [00:11<00:00, 18.31it/s, loss=1.29, v_num=59, test_loss=1.400]And now abuse. His time:\n",
      "More: this:\n",
      "With request inter'd but your sist\n",
      "Epoch 24: 100%|██████████| 209/209 [00:11<00:00, 17.97it/s, loss=1.29, v_num=59, test_loss=1.410]And now\n",
      "Whilst I shouldst defence, for the women'd to set of thee heave\n",
      "Epoch 24: 100%|██████████| 209/209 [00:12<00:00, 16.44it/s, loss=1.29, v_num=59, test_loss=1.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:12<00:00, 16.38it/s, loss=1.29, v_num=59, test_loss=1.420]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        v = self.v(F.pad(xn, (0, 0, 1, -1)))\n",
    "        attn = self.proj(torch.sigmoid(q) * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "import lightning as pl\n",
    "import torch.utils.data as data\n",
    "device = \"mps\"\n",
    "batch_size = 36\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor(tokenizer.encode(str).ids, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #out = self(ids[:, -block_size:])\n",
    "            out = self(ids)\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = Model(tokenizer.get_vocab_size())\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! what wise, inconspirator:\n",
      "And thy warrant. Somerset a pleased, only till you have not frowns are them the period to do nor hands upon, this so the sanst me to his heart will, and him, my lord's say 'Ay.\n",
      "I authority:\n",
      "If I means over business?\n",
      "And woodfellow? where's my honour, much,\n",
      "Have your garpends the searchs disgraces may should sleep out of your gentlemen,--\n",
      "\n",
      "QUEEN:\n",
      "All the writ, a month the whose my face;\n",
      "Saddle will chide, thus. He do you better sleep himself in such a dream'd,\n",
      "I have, I would done;\n",
      "We must some presence as at his sudden so beseech you?\n",
      "\n",
      "RIVERS:\n",
      "How so still hold, man?\n",
      "Once the fires to the daughters o' the blood\n",
      "Hast\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
