{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 885.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "block_size = 256\n",
    "test_size = 1500\n",
    "batch_size = 36\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - test_size))\n",
    "test = data.Subset(dataset, range(len(dataset) - test_size, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 190 K \n",
      "-------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 38.46it/s]And nowQvt-\n",
      "M HH?twa:xpoMCjPD.XoSCDZhFy:qVyGRHHnQCjAM !DURnHnMtNwpXqNQ!\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 209/209 [00:05<00:00, 36.60it/s, loss=1.66, v_num=65]And now thy munius offistick things it, tell of a grefore it?\n",
      "\n",
      "AUTIO:\n",
      "H\n",
      "Epoch 1: 100%|██████████| 209/209 [00:05<00:00, 38.75it/s, loss=1.52, v_num=65, test_loss=1.880]And now as it imposencius;\n",
      "It new my will be softh: thus! a put, and by\n",
      "Epoch 2: 100%|██████████| 209/209 [00:05<00:00, 39.03it/s, loss=1.46, v_num=65, test_loss=1.740]And now,\n",
      "Which not.\n",
      "If I hiv those here than I must with to they shadow\n",
      "Epoch 3: 100%|██████████| 209/209 [00:05<00:00, 39.12it/s, loss=1.43, v_num=65, test_loss=1.630]And now\n",
      "The world mayler's discolds,\n",
      "And self brief;\n",
      "And here is feeds \n",
      "Epoch 4: 100%|██████████| 209/209 [00:05<00:00, 39.55it/s, loss=1.41, v_num=65, test_loss=1.550]And now not faith of than I speaks?\n",
      "Why stand where a wista an ere I cu\n",
      "Epoch 5: 100%|██████████| 209/209 [00:05<00:00, 38.50it/s, loss=1.39, v_num=65, test_loss=1.530]And now I had widow at there a say;\n",
      "After: he thrust thou, that stand a\n",
      "Epoch 6: 100%|██████████| 209/209 [00:05<00:00, 38.59it/s, loss=1.38, v_num=65, test_loss=1.530]And now the world,\n",
      "All shout:\n",
      "Well,\n",
      "And hence.\n",
      "Thou lie.\n",
      "\n",
      "LARTIUS:\n",
      "No, \n",
      "Epoch 7: 100%|██████████| 209/209 [00:05<00:00, 38.58it/s, loss=1.37, v_num=65, test_loss=1.490]And now.\n",
      "\n",
      "First Musician; but, she world them;\n",
      "And I.\n",
      "\n",
      "MENENIUS:\n",
      "Not to\n",
      "Epoch 8: 100%|██████████| 209/209 [00:05<00:00, 38.55it/s, loss=1.37, v_num=65, test_loss=1.460]And now and leave her our\n",
      "straight.\n",
      "\n",
      "COMINIUS:\n",
      "I were is shall provinou\n",
      "Epoch 9: 100%|██████████| 209/209 [00:05<00:00, 38.63it/s, loss=1.35, v_num=65, test_loss=1.460]And now in my gight.\n",
      "\n",
      "SICINIUS:\n",
      "It is it will do not sullen thou shall \n",
      "Epoch 10: 100%|██████████| 209/209 [00:05<00:00, 38.82it/s, loss=1.35, v_num=65, test_loss=1.460]And now, to cheque of you tut my sound for their from this life?\n",
      "\n",
      "POLIX\n",
      "Epoch 11: 100%|██████████| 209/209 [00:05<00:00, 38.66it/s, loss=1.34, v_num=65, test_loss=1.440]And now fair worthy prince\n",
      "Both witness of thy commandmake your captain\n",
      "Epoch 12: 100%|██████████| 209/209 [00:05<00:00, 38.20it/s, loss=1.34, v_num=65, test_loss=1.460]And now the way the hours are to him;\n",
      "If I lie maid,\n",
      "Because myself.\n",
      "He\n",
      "Epoch 13: 100%|██████████| 209/209 [00:05<00:00, 38.14it/s, loss=1.33, v_num=65, test_loss=1.460]And now who,\n",
      "Whom she had back; that have able shalt not war.\n",
      "Were it c\n",
      "Epoch 14: 100%|██████████| 209/209 [00:05<00:00, 38.07it/s, loss=1.32, v_num=65, test_loss=1.440]And now, but of the world. Thereford to thy father the would not to his\n",
      "Epoch 15: 100%|██████████| 209/209 [00:05<00:00, 37.74it/s, loss=1.32, v_num=65, test_loss=1.430]And now now my time in the married in steel to hear,\n",
      "Affray, the days, \n",
      "Epoch 16: 100%|██████████| 209/209 [00:05<00:00, 37.92it/s, loss=1.31, v_num=65, test_loss=1.430]And now.\n",
      "\n",
      "A Woanners are a must be gap him; I'll prosperous for me; som\n",
      "Epoch 17: 100%|██████████| 209/209 [00:05<00:00, 37.72it/s, loss=1.31, v_num=65, test_loss=1.440]And now thou sworn and bloody doth more.\n",
      "\n",
      "Port, and hear my body's news\n",
      "Epoch 18: 100%|██████████| 209/209 [00:05<00:00, 36.92it/s, loss=1.3, v_num=65, test_loss=1.420] And now and so too lads it way.\n",
      "I hope.\n",
      "\n",
      "First Murder of that then, I k\n",
      "Epoch 19: 100%|██████████| 209/209 [00:05<00:00, 36.98it/s, loss=1.3, v_num=65, test_loss=1.430] And now have a stand help indeed, my lambs and all be in strattend,\n",
      "To \n",
      "Epoch 20: 100%|██████████| 209/209 [00:05<00:00, 37.02it/s, loss=1.3, v_num=65, test_loss=1.430]And now out of himself\n",
      "We settle:\n",
      "I'll realm,\n",
      "He stermane by one, for I\n",
      "Epoch 21: 100%|██████████| 209/209 [00:05<00:00, 35.49it/s, loss=1.31, v_num=65, test_loss=1.430]And now.\n",
      "\n",
      "BIONDELLO:\n",
      "Yea, and the great and well, I dreams!\n",
      "A bawd.\n",
      "\n",
      "SA\n",
      "Epoch 22: 100%|██████████| 209/209 [00:06<00:00, 32.31it/s, loss=1.29, v_num=65, test_loss=1.400]And now.\n",
      "\n",
      "Abate of the field the turn thy sound;\n",
      "While.\n",
      "Well, and thou \n",
      "Epoch 23: 100%|██████████| 209/209 [00:06<00:00, 29.94it/s, loss=1.29, v_num=65, test_loss=1.410]And now bird Citizen:\n",
      "Speak of his face,\n",
      "That to tirs we be honour, and\n",
      "Epoch 24: 100%|██████████| 209/209 [00:07<00:00, 27.94it/s, loss=1.29, v_num=65, test_loss=1.410]And now will them nothings.\n",
      "\n",
      "First Senators:\n",
      "So the gave found soor pro\n",
      "Epoch 24: 100%|██████████| 209/209 [00:08<00:00, 25.67it/s, loss=1.29, v_num=65, test_loss=1.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:08<00:00, 25.56it/s, loss=1.29, v_num=65, test_loss=1.420]\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        v = self.v(F.pad(xn, (0, 0, 1, -1)))\n",
    "        attn = self.proj(torch.sigmoid(q) * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor([dataset.stoi[ch] for ch in str], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return \"\".join([dataset.itos[int(i)] for i in ids[0]])\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for b in model.model.transformer[1]:\n",
    "    plt.figure()\n",
    "    plt.imshow(b.q.weight.detach())\n",
    "    plt.figure()\n",
    "    plt.imshow(b.mlp[0].weight.detach())\n",
    "    plt.figure()\n",
    "    plt.imshow(b.mlp[2].weight.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(\"O God, O God!\".rjust(block_size), 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
