{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1282.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from shared import corpus, tokenizers, datasets\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "train, test = datasets.causal(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 190 K \n",
      "-------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 29.56it/s]And now:cncvL?pZElbQ3pjoxbgvX-?\n",
      "vgC:GRzjyWgUBO3OchYRy,F d3tKI,F qsQhKZV\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 209/209 [00:05<00:00, 35.85it/s, loss=1.67, v_num=43]And now, what here's for it would stop are and with the paice my fair,\n",
      "\n",
      "Epoch 1: 100%|██████████| 209/209 [00:05<00:00, 37.97it/s, loss=1.53, v_num=43, test_loss=1.830]And now, to disdang althen\n",
      "And by him.\n",
      "\n",
      "GLOUCESTER:\n",
      "Go things, it is su\n",
      "Epoch 2: 100%|██████████| 209/209 [00:06<00:00, 30.04it/s, loss=1.46, v_num=43, test_loss=1.680]And now his part actions the mounty!'\n",
      "This set the grounds\n",
      "Will hence,\n",
      "\n",
      "Epoch 3: 100%|██████████| 209/209 [00:08<00:00, 25.45it/s, loss=1.43, v_num=43, test_loss=1.590]And now If the pray yonderor him bector outs back the captions to store\n",
      "Epoch 4: 100%|██████████| 209/209 [00:09<00:00, 20.97it/s, loss=1.41, v_num=43, test_loss=1.530]And now?\n",
      "\n",
      "MENENIUS:\n",
      "I will be gry brother,\n",
      "And shall spurpose.\n",
      "\n",
      "GLOUCES\n",
      "Epoch 5: 100%|██████████| 209/209 [00:11<00:00, 18.71it/s, loss=1.39, v_num=43, test_loss=1.540]And now, ho! what, methought in mind of my son\n",
      "Is but the sorne at a ch\n",
      "Epoch 6: 100%|██████████| 209/209 [00:11<00:00, 18.98it/s, loss=1.38, v_num=43, test_loss=1.510]And now?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Then I\n",
      "morning the that your fintal approfane\n",
      "Epoch 7: 100%|██████████| 209/209 [00:10<00:00, 19.77it/s, loss=1.37, v_num=43, test_loss=1.490]And now,\n",
      "Against me steeds as hither, this and fellow our soundless my \n",
      "Epoch 8: 100%|██████████| 209/209 [00:09<00:00, 22.76it/s, loss=1.35, v_num=43, test_loss=1.460]And now a tack, I peage to pink for min with should be seek answer seve\n",
      "Epoch 9: 100%|██████████| 209/209 [00:09<00:00, 21.58it/s, loss=1.35, v_num=43, test_loss=1.450]And now now, boy?\n",
      "\n",
      "Ghost no between\n",
      "Of word\n",
      "When your monstrumpet it!\n",
      "\n",
      "\n",
      "Epoch 10: 100%|██████████| 209/209 [00:08<00:00, 23.27it/s, loss=1.35, v_num=43, test_loss=1.470]And now too rarers.\n",
      "\n",
      "First Murder, as his,\n",
      "And if is a basern arm.\n",
      "\n",
      "Sec\n",
      "Epoch 11: 100%|██████████| 209/209 [00:08<00:00, 23.48it/s, loss=1.34, v_num=43, test_loss=1.440]And now thou have beauted\n",
      "Would not pured by witness in a word.\n",
      "\n",
      "SICINI\n",
      "Epoch 12: 100%|██████████| 209/209 [00:09<00:00, 21.13it/s, loss=1.34, v_num=43, test_loss=1.460]And now, sir, which her have disperses: it was not thy find tell that t\n",
      "Epoch 13: 100%|██████████| 209/209 [00:08<00:00, 23.55it/s, loss=1.34, v_num=43, test_loss=1.450]And now might an uncle?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "This world-a-direct it, that w\n",
      "Epoch 14: 100%|██████████| 209/209 [00:08<00:00, 25.69it/s, loss=1.33, v_num=43, test_loss=1.410]And now I play'd in my breathe temporance in all.\n",
      "\n",
      "COMINIUS:\n",
      "I purpious\n",
      "Epoch 15: 100%|██████████| 209/209 [00:08<00:00, 23.26it/s, loss=1.32, v_num=43, test_loss=1.430]And now with save other! the water our come would have your. My faces s\n",
      "Epoch 16: 100%|██████████| 209/209 [00:09<00:00, 23.20it/s, loss=1.31, v_num=43, test_loss=1.420]And now fardel\n",
      "What men have so far issue.\n",
      "\n",
      "KING EDWARD III:\n",
      "Never this\n",
      "Epoch 17: 100%|██████████| 209/209 [00:09<00:00, 22.68it/s, loss=1.31, v_num=43, test_loss=1.400]And now stay with some is they are thee! thou love you had too, and he \n",
      "Epoch 18: 100%|██████████| 209/209 [00:09<00:00, 23.18it/s, loss=1.31, v_num=43, test_loss=1.420]And now his wime hatch.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Some charited men; whom to bea\n",
      "Epoch 19: 100%|██████████| 209/209 [00:09<00:00, 22.89it/s, loss=1.3, v_num=43, test_loss=1.420] And now thy hand on and still true, I come till note,\n",
      "From obstinity, a\n",
      "Epoch 20: 100%|██████████| 209/209 [00:09<00:00, 22.86it/s, loss=1.31, v_num=43, test_loss=1.410]And now it, will be white and by\n",
      "Is brother.--Seedings, and to call our\n",
      "Epoch 21: 100%|██████████| 209/209 [00:09<00:00, 22.99it/s, loss=1.3, v_num=43, test_loss=1.430] And now believed a cause; for his holes,\n",
      "Will I belike.\n",
      "\n",
      "POLIXENES:\n",
      "Non\n",
      "Epoch 22: 100%|██████████| 209/209 [00:09<00:00, 22.60it/s, loss=1.29, v_num=43, test_loss=1.410]And now these army\n",
      "An\n",
      "In the fees,\n",
      "And sinew the wounds:\n",
      "By sovereign.\n",
      "\n",
      "Epoch 23: 100%|██████████| 209/209 [00:09<00:00, 22.66it/s, loss=1.29, v_num=43, test_loss=1.400]And now too, a sorrow'd the prince to-day is no longer, what need\n",
      "By Lo\n",
      "Epoch 24: 100%|██████████| 209/209 [00:09<00:00, 23.12it/s, loss=1.29, v_num=43, test_loss=1.410]And now and a beggar her hold all\n",
      "That doers or writ to pardon.\n",
      "\n",
      "DUKE V\n",
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 20.84it/s, loss=1.29, v_num=43, test_loss=1.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 209/209 [00:10<00:00, 20.74it/s, loss=1.29, v_num=43, test_loss=1.420]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        v = self.v(F.pad(xn, (0, 0, 1, -1)))\n",
    "        attn = self.proj(torch.sigmoid(q) * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "import lightning as pl\n",
    "import torch.utils.data as data\n",
    "device = \"mps\"\n",
    "batch_size = 36\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.007):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = GPT(vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=batch_size, num_workers=0, sampler=data.RandomSampler(train, False, 6_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            print(self.generate(\"And now\", 64))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.95, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "    # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, str, max_new_tokens, top_k=10):\n",
    "        ids = torch.tensor(tokenizer.encode(str), dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #out = self(ids[:, -block_size:])\n",
    "            out = self(ids)\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            ids = torch.cat((ids, step_res), dim=1)\n",
    "        return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = Model(tokenizer.vocab_size)\n",
    "trainer = pl.Trainer(max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else device)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! what wise, inconspirator:\n",
      "And thy warrant. Somerset a pleased, only till you have not frowns are them the period to do nor hands upon, this so the sanst me to his heart will, and him, my lord's say 'Ay.\n",
      "I authority:\n",
      "If I means over business?\n",
      "And woodfellow? where's my honour, much,\n",
      "Have your garpends the searchs disgraces may should sleep out of your gentlemen,--\n",
      "\n",
      "QUEEN:\n",
      "All the writ, a month the whose my face;\n",
      "Saddle will chide, thus. He do you better sleep himself in such a dream'd,\n",
      "I have, I would done;\n",
      "We must some presence as at his sudden so beseech you?\n",
      "\n",
      "RIVERS:\n",
      "How so still hold, man?\n",
      "Once the fires to the daughters o' the blood\n",
      "Hast\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
