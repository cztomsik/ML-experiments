{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=256, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        v = self.v(F.pad(xn, (0, 0, 1, -1)))\n",
    "        attn = self.proj(torch.sigmoid(q) * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 870.31it/s]\n",
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 190 K \n",
      "-------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 26.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And nowS3c'LKXz z LSjinrFlEd3;.nkUy\n",
      "TG;.l;hrdstZzOoxQ bazjo.-kHmTjetKHx\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 202/202 [00:05<00:00, 34.90it/s, loss=1.67, v_num=1]And now a goods mench'd the who he misdess'd as and forgut the preper a\n",
      "Epoch 1: 100%|██████████| 202/202 [00:05<00:00, 36.92it/s, loss=1.53, v_num=1, test_loss=1.830]And now\n",
      "His to him inspeed\n",
      "The mad see,\n",
      "And noblest so lights\n",
      "Shring hi\n",
      "Epoch 2: 100%|██████████| 202/202 [00:05<00:00, 34.73it/s, loss=1.46, v_num=1, test_loss=1.670]And now, if you had his sposite my father home, there means\n",
      "Than thy br\n",
      "Epoch 3: 100%|██████████| 202/202 [00:05<00:00, 36.86it/s, loss=1.43, v_num=1, test_loss=1.580]And now the crow no fench,\n",
      "Speak not their ladies in ass,\n",
      "So he wifesti\n",
      "Epoch 4: 100%|██████████| 202/202 [00:05<00:00, 38.39it/s, loss=1.41, v_num=1, test_loss=1.510]And now too, slept?\n",
      "\n",
      "First Musician:\n",
      "Then to you?\n",
      "\n",
      "DORCAS:\n",
      "What now.\n",
      "\n",
      "B\n",
      "Epoch 5: 100%|██████████| 202/202 [00:05<00:00, 38.13it/s, loss=1.39, v_num=1, test_loss=1.530]And now; and doth son my bawd allies,\n",
      "And thy tarken as a marry, while:\n",
      "Epoch 6: 100%|██████████| 202/202 [00:05<00:00, 38.17it/s, loss=1.38, v_num=1, test_loss=1.490]And now my brother should to have all stol'n this so wretch as if we do\n",
      "Epoch 7: 100%|██████████| 202/202 [00:05<00:00, 37.88it/s, loss=1.37, v_num=1, test_loss=1.480]And now I'll say 'Country 'Is\n",
      "Call heart she distander'd to bite thou w\n",
      "Epoch 8: 100%|██████████| 202/202 [00:05<00:00, 37.86it/s, loss=1.35, v_num=1, test_loss=1.450]And now and trut, what do you but a woman, my lord, I would not do the \n",
      "Epoch 9: 100%|██████████| 202/202 [00:05<00:00, 37.73it/s, loss=1.35, v_num=1, test_loss=1.430]And now such a son an any most.\n",
      "\n",
      "KING RICHARD IV:\n",
      "Now sudden remorsere \n",
      "Epoch 10: 100%|██████████| 202/202 [00:05<00:00, 37.45it/s, loss=1.35, v_num=1, test_loss=1.460]And now not\n",
      "Have and an our cuitors had then, then;\n",
      "And that, sail and \n",
      "Epoch 11: 100%|██████████| 202/202 [00:05<00:00, 37.25it/s, loss=1.34, v_num=1, test_loss=1.430]And now,\n",
      "And well a father arms.\n",
      "\n",
      "POMPEY:\n",
      "O, for it from his seems must\n",
      "Epoch 12: 100%|██████████| 202/202 [00:05<00:00, 37.05it/s, loss=1.34, v_num=1, test_loss=1.450]And now he beamentable hear; but here. I hief; throw the fellows in my \n",
      "Epoch 13: 100%|██████████| 202/202 [00:05<00:00, 36.65it/s, loss=1.34, v_num=1, test_loss=1.430]And nowhere I be aband,\n",
      "To fear of a world!\n",
      "You art of not first,\n",
      "Yault\n",
      "Epoch 14: 100%|██████████| 202/202 [00:05<00:00, 35.79it/s, loss=1.33, v_num=1, test_loss=1.400]And now, in my faining whose grace and limber'd me.\n",
      "One is thy secret a\n",
      "Epoch 15: 100%|██████████| 202/202 [00:05<00:00, 36.66it/s, loss=1.32, v_num=1, test_loss=1.410]And now the told my son into ask, if well down,\n",
      "By answer'd and my moth\n",
      "Epoch 16: 100%|██████████| 202/202 [00:05<00:00, 36.33it/s, loss=1.31, v_num=1, test_loss=1.410]And now\n",
      "Her brother woman?\n",
      "And to my sacred\n",
      "But to diess of our hour we\n",
      "Epoch 17: 100%|██████████| 202/202 [00:05<00:00, 36.59it/s, loss=1.31, v_num=1, test_loss=1.390]And now sometime, I did no seation: what help this shriek'd hiss me not\n",
      "Epoch 18: 100%|██████████| 202/202 [00:05<00:00, 36.30it/s, loss=1.31, v_num=1, test_loss=1.420]And now a man of a for a break with\n",
      "you both frowns for thou, nobles, t\n",
      "Epoch 19: 100%|██████████| 202/202 [00:05<00:00, 36.59it/s, loss=1.3, v_num=1, test_loss=1.410] And now:\n",
      "A lambs at, sir.\n",
      "\n",
      "KING LEWIS XI:\n",
      "He? what haste\n",
      "thousand if it\n",
      "Epoch 20: 100%|██████████| 202/202 [00:05<00:00, 34.37it/s, loss=1.31, v_num=1, test_loss=1.400]And now it patrician roans,\n",
      "Makes of many sure any match'd: I am hence \n",
      "Epoch 21: 100%|██████████| 202/202 [00:06<00:00, 30.92it/s, loss=1.3, v_num=1, test_loss=1.420] And now he with has he my sense; then them sleeping hate, i' from mine \n",
      "Epoch 22: 100%|██████████| 202/202 [00:06<00:00, 30.34it/s, loss=1.29, v_num=1, test_loss=1.400]And now, and me, breasted too doth all them.\n",
      "\n",
      "MONTAGUE:\n",
      "So, before he c\n",
      "Epoch 23: 100%|██████████| 202/202 [00:07<00:00, 28.20it/s, loss=1.29, v_num=1, test_loss=1.400]And now I will shadow fair of his full of his borne than it never in al\n",
      "Epoch 24: 100%|██████████| 202/202 [00:07<00:00, 26.64it/s, loss=1.29, v_num=1, test_loss=1.400]And now\n",
      "Looking, for which out often foot and he is the provost:\n",
      "As ima\n",
      "Epoch 24: 100%|██████████| 202/202 [00:08<00:00, 24.57it/s, loss=1.29, v_num=1, test_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 202/202 [00:08<00:00, 24.46it/s, loss=1.29, v_num=1, test_loss=1.410]\n"
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "from shared import corpus, tokenizers, trainers\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = GPT(tokenizer.get_vocab_size())\n",
    "trainer = trainers.CausalTrainer(model, tokenizer, device = \"mps\")\n",
    "trainer.train(text, batch_size=36, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! what wise, inconspirator:\n",
      "And thy warrant. Somerset a pleased, only till you have not frowns are them the period to do nor hands upon, this so the sanst me to his heart will, and him, my lord's say 'Ay.\n",
      "I authority:\n",
      "If I means over business?\n",
      "And woodfellow? where's my honour, much,\n",
      "Have your garpends the searchs disgraces may should sleep out of your gentlemen,--\n",
      "\n",
      "QUEEN:\n",
      "All the writ, a month the whose my face;\n",
      "Saddle will chide, thus. He do you better sleep himself in such a dream'd,\n",
      "I have, I would done;\n",
      "We must some presence as at his sudden so beseech you?\n",
      "\n",
      "RIVERS:\n",
      "How so still hold, man?\n",
      "Once the fires to the daughters o' the blood\n",
      "Hast\n"
     ]
    }
   ],
   "source": [
    "print(trainer.wrapper.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
