{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=256, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Sequential(*[Layer(embed_dim) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xn = self.ln1(x)\n",
    "        q = self.q(xn)\n",
    "        v = self.v(F.pad(xn, (0, 0, 1, -1)))\n",
    "        attn = self.proj(torch.sigmoid(q) * v)\n",
    "\n",
    "        x = x + attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 685.12it/s]\n",
      "Global seed set to 89026614\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 190 K \n",
      "-------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 30.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now;h'$lp:RkkSSimo3mqMoQeP-$o.FRet:N3Oy\n",
      "nkSSx'XjXXXeWs bqMZ-fbASQ:g\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/envs/torch-mps/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 202/202 [00:05<00:00, 34.14it/s, loss=1.67, v_num=67]And now from hath if thee opein to a propharch the desile hand for stee\n",
      "Epoch 1: 100%|██████████| 202/202 [00:05<00:00, 35.94it/s, loss=1.53, v_num=67, test_loss=1.830]And now,\n",
      "And she's a cold in prays:\n",
      "And me, as a too sorried.\n",
      "\n",
      "CORIOLAN\n",
      "Epoch 2: 100%|██████████| 202/202 [00:05<00:00, 37.07it/s, loss=1.46, v_num=67, test_loss=1.670]And now have let morrow me himself, betranger, by a phose slain staunt \n",
      "Epoch 3: 100%|██████████| 202/202 [00:05<00:00, 36.99it/s, loss=1.43, v_num=67, test_loss=1.580]And now you drat a man, and her my heart\n",
      "Commities offer one ancient we\n",
      "Epoch 4: 100%|██████████| 202/202 [00:05<00:00, 37.23it/s, loss=1.41, v_num=67, test_loss=1.510]And now so silver me in my gracious, intertake this sword is this so fe\n",
      "Epoch 5: 100%|██████████| 202/202 [00:05<00:00, 36.67it/s, loss=1.39, v_num=67, test_loss=1.530]And now, he is agrew to the wurld think upon he doth noble his danny hi\n",
      "Epoch 6: 100%|██████████| 202/202 [00:05<00:00, 37.20it/s, loss=1.38, v_num=67, test_loss=1.490]And now the been-a stand by speech. God to than I\n",
      "To sell the face, bre\n",
      "Epoch 7: 100%|██████████| 202/202 [00:05<00:00, 37.59it/s, loss=1.37, v_num=67, test_loss=1.480]And now the beg of as a presently bade the seated slemition my chool?\n",
      "I\n",
      "Epoch 8: 100%|██████████| 202/202 [00:05<00:00, 37.39it/s, loss=1.35, v_num=67, test_loss=1.450]And now of my father's come my lies fit, I do and for subject\n",
      "Thou have\n",
      "Epoch 9: 100%|██████████| 202/202 [00:05<00:00, 36.82it/s, loss=1.35, v_num=67, test_loss=1.430]And now no more, and\n",
      "sound herd:\n",
      "Not a monster, little belier,\n",
      "We have \n",
      "Epoch 10: 100%|██████████| 202/202 [00:05<00:00, 36.95it/s, loss=1.35, v_num=67, test_loss=1.460]And now what happy will thy could have thee? then\n",
      "Art he daughter, spea\n",
      "Epoch 11: 100%|██████████| 202/202 [00:05<00:00, 36.18it/s, loss=1.34, v_num=67, test_loss=1.430]And now I with made broke's this, I'll several where should not fairly \n",
      "Epoch 12: 100%|██████████| 202/202 [00:05<00:00, 36.69it/s, loss=1.34, v_num=67, test_loss=1.450]And now, sir.\n",
      "A persed but some well see that are now the bastard\n",
      "Than \n",
      "Epoch 13: 100%|██████████| 202/202 [00:05<00:00, 36.81it/s, loss=1.34, v_num=67, test_loss=1.430]And now\n",
      "His life,\n",
      "And learn\n",
      "And weeping stoop the people fourth,\n",
      "But th\n",
      "Epoch 14: 100%|██████████| 202/202 [00:05<00:00, 36.25it/s, loss=1.33, v_num=67, test_loss=1.400]And now. For the cloud,\n",
      "Honest him for successor, and\n",
      "The way have dete\n",
      "Epoch 15: 100%|██████████| 202/202 [00:05<00:00, 36.42it/s, loss=1.32, v_num=67, test_loss=1.410]And now I be my lie.\n",
      "\n",
      "LEONTES:\n",
      "I draw\n",
      "A man, there:\n",
      "And talk'd his life\n",
      "Epoch 16: 100%|██████████| 202/202 [00:05<00:00, 36.34it/s, loss=1.31, v_num=67, test_loss=1.410]And now in all on yourse to the that canst my hand\n",
      "The most did be acco\n",
      "Epoch 17: 100%|██████████| 202/202 [00:05<00:00, 36.39it/s, loss=1.31, v_num=67, test_loss=1.390]And now that a sins here, we tears! The movednesday your please\n",
      "To brou\n",
      "Epoch 18: 100%|██████████| 202/202 [00:05<00:00, 36.05it/s, loss=1.31, v_num=67, test_loss=1.420]And now in the way? Tell heart's the were we than so mournerative his.\n",
      "\n",
      "Epoch 19: 100%|██████████| 202/202 [00:05<00:00, 35.72it/s, loss=1.3, v_num=67, test_loss=1.410] And now any that do her with a more stings, by my true suffice:\n",
      "There's\n",
      "Epoch 20: 100%|██████████| 202/202 [00:05<00:00, 35.98it/s, loss=1.31, v_num=67, test_loss=1.400]And now shall not sell; for him. There,\n",
      "To dies; therein?\n",
      "I'll the face\n",
      "Epoch 21: 100%|██████████| 202/202 [00:05<00:00, 35.97it/s, loss=1.3, v_num=67, test_loss=1.420] And now? thy heard that sure? what thou hast beguile to answer; anon,\n",
      "A\n",
      "Epoch 22: 100%|██████████| 202/202 [00:05<00:00, 36.13it/s, loss=1.29, v_num=67, test_loss=1.400]And now beseech you?\n",
      "That any a little wear to changed may say now the \n",
      "Epoch 23: 100%|██████████| 202/202 [00:05<00:00, 35.32it/s, loss=1.29, v_num=67, test_loss=1.400]And now:\n",
      "An honour is a perish as their poor.\n",
      "My lord; and watch:\n",
      "Glad \n",
      "Epoch 24: 100%|██████████| 202/202 [00:05<00:00, 35.20it/s, loss=1.29, v_num=67, test_loss=1.400]And now his drink'st usurp and leave you:\n",
      "This fault; and deadly and th\n",
      "Epoch 24: 100%|██████████| 202/202 [00:06<00:00, 32.89it/s, loss=1.29, v_num=67, test_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 202/202 [00:06<00:00, 32.72it/s, loss=1.29, v_num=67, test_loss=1.410]\n"
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "from shared import corpus, tokenizers, trainers\n",
    "\n",
    "text = corpus.shakespeare()\n",
    "tokenizer = tokenizers.unique_chars(text)\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "model = GPT(tokenizer.get_vocab_size())\n",
    "trainer = trainers.CausalTrainer(model, tokenizer, device = \"mps\")\n",
    "trainer.train(text, batch_size=36, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! what wise, inconspirator:\n",
      "And thy warrant. Somerset a pleased, only till you have not frowns are them the period to do nor hands upon, this so the sanst me to his heart will, and him, my lord's say 'Ay.\n",
      "I authority:\n",
      "If I means over business?\n",
      "And woodfellow? where's my honour, much,\n",
      "Have your garpends the searchs disgraces may should sleep out of your gentlemen,--\n",
      "\n",
      "QUEEN:\n",
      "All the writ, a month the whose my face;\n",
      "Saddle will chide, thus. He do you better sleep himself in such a dream'd,\n",
      "I have, I would done;\n",
      "We must some presence as at his sudden so beseech you?\n",
      "\n",
      "RIVERS:\n",
      "How so still hold, man?\n",
      "Once the fires to the daughters o' the blood\n",
      "Hast\n"
     ]
    }
   ],
   "source": [
    "print(trainer.wrapper.generate(\"O God, O God!\", 650))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b771c8f4755fba7026a17ac6ea0287a4709b89559dd079886423c614dfd53ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
