{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 89026614\n",
      "Found cached dataset tiny_shakespeare (/Users/cztomsik/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 887.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "pl.seed_everything(89026614)\n",
    "\n",
    "text = datasets.load_dataset('tiny_shakespeare')[\"train\"][0][\"text\"]\n",
    "#text = open('../../Downloads/simplebooks/simplebooks-2-raw/train.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "block_size = 100\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        super().__init__()\n",
    "        vocab = sorted(set(text))\n",
    "        self.vocab = vocab\n",
    "        self.stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) - block_size - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        end = i + block_size\n",
    "        return self.data[i:end], self.data[i + 1:end + 1]\n",
    "\n",
    "dataset = MyDataset(text)\n",
    "train = data.Subset(dataset, range(0, len(dataset) - 500))\n",
    "test = data.Subset(dataset, range(len(dataset) - 500, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | MinGPT | 1.4 M \n",
      "---------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.560     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 36.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now -k\n",
      "B&Y,upRo&tjRQzvYFk\n",
      "R.\n",
      "AEVpAz'KCi3vLQqKCrVUDya.VGgAl yIhduObgH\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cztomsik/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 82/82 [00:20<00:00,  4.03it/s, loss=2.46, v_num=98]And now a mitown h ome s ant toome s moueeted at mur aith wir imerot tha\n",
      "Epoch 1: 100%|██████████| 82/82 [00:20<00:00,  3.97it/s, loss=2.31, v_num=98, test_loss=2.400]And now sot oursint th my\n",
      "The timy, se wherur thacuseeng t sponon the.\n",
      "T\n",
      "Epoch 2: 100%|██████████| 82/82 [00:28<00:00,  2.90it/s, loss=2.08, v_num=98, test_loss=2.220]And now a the my mushirs.\n",
      "\n",
      "MEYORDO:\n",
      "Ay, my hard,\n",
      "And is in will of whe a\n",
      "Epoch 3: 100%|██████████| 82/82 [00:31<00:00,  2.61it/s, loss=1.92, v_num=98, test_loss=2.140]And now that stow.\n",
      "\n",
      "MARGAMIA:\n",
      "Will may my hime anesulve to dight him see\n",
      "Epoch 4: 100%|██████████| 82/82 [00:24<00:00,  3.29it/s, loss=1.79, v_num=98, test_loss=1.920]And now belave teing hath my to cone.\n",
      "\n",
      "DUKE OF YORK:\n",
      "With him bod you, h\n",
      "Epoch 5: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.71, v_num=98, test_loss=1.840]And now them. Willow no shall boody?\n",
      "\n",
      "DUKE OF YORK:\n",
      "No, for tell. Sir, w\n",
      "Epoch 6: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.66, v_num=98, test_loss=1.730]And now mark of thee,\n",
      "But we with sink. He's will hold that she\n",
      "truncle \n",
      "Epoch 7: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.62, v_num=98, test_loss=1.640]And now to ten is down: should will be it went\n",
      "On that trutto all my han\n",
      "Epoch 8: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.59, v_num=98, test_loss=1.610]And now all, who swell his leave high hows,\n",
      "The bid the distrable bands \n",
      "Epoch 9: 100%|██████████| 82/82 [00:23<00:00,  3.45it/s, loss=1.56, v_num=98, test_loss=1.590]And now been all fiends!\n",
      "I do must be so me her servaliarions:\n",
      "Then they\n",
      "Epoch 10: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.54, v_num=98, test_loss=1.570]And now one too, my to mine a driff\n",
      "And mispressomes a confolk.\n",
      "\n",
      "ButABEL\n",
      "Epoch 11: 100%|██████████| 82/82 [00:23<00:00,  3.43it/s, loss=1.52, v_num=98, test_loss=1.530]And now not slay they have slay the doster,\n",
      "Most our factive this one to\n",
      "Epoch 12: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.5, v_num=98, test_loss=1.470] And now with with the cromiss she?\n",
      "\n",
      "First Gentleman:\n",
      "He can hath shall b\n",
      "Epoch 13: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.48, v_num=98, test_loss=1.490]And now thy friar the feather spokens\n",
      "A boder'd of me, from there's both\n",
      "Epoch 14: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.47, v_num=98, test_loss=1.470]And now with to be men, by seem to furry to so him\n",
      "To that thee, as noth\n",
      "Epoch 15: 100%|██████████| 82/82 [00:23<00:00,  3.47it/s, loss=1.46, v_num=98, test_loss=1.480]And now how barried of the tribuitiors shame.\n",
      "Thou tell the him thou hat\n",
      "Epoch 16: 100%|██████████| 82/82 [00:23<00:00,  3.49it/s, loss=1.46, v_num=98, test_loss=1.470]And now thou say my prividers have day a\n",
      "seek to be men a supping the ce\n",
      "Epoch 17: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.44, v_num=98, test_loss=1.450]And now the woman are welling stand\n",
      "Shall will thou lost tearsh and true\n",
      "Epoch 18: 100%|██████████| 82/82 [00:23<00:00,  3.45it/s, loss=1.44, v_num=98, test_loss=1.470]And now me my care itsolured\n",
      "That it to makes nothing o'er ignoble treas\n",
      "Epoch 19: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.43, v_num=98, test_loss=1.420]And now my how that: besides,\n",
      "Your work to make to prompair than I too.\n",
      "\n",
      "Epoch 20: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.42, v_num=98, test_loss=1.420]And now my father; I say you content thee to feel thought\n",
      "Your heaven mo\n",
      "Epoch 21: 100%|██████████| 82/82 [00:23<00:00,  3.47it/s, loss=1.42, v_num=98, test_loss=1.410]And now he did no mother further of death,\n",
      "And in thus ire this war true\n",
      "Epoch 22: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.41, v_num=98, test_loss=1.390]And now at me and shoest sentencession\n",
      "And fetch: trom there is a thours\n",
      "Epoch 23: 100%|██████████| 82/82 [00:23<00:00,  3.46it/s, loss=1.41, v_num=98, test_loss=1.370]And now holp'd, if soul, I must contrant,\n",
      "As so any those traitor on my \n",
      "Epoch 24: 100%|██████████| 82/82 [00:23<00:00,  3.47it/s, loss=1.4, v_num=98, test_loss=1.410] And now we stay for meet,\n",
      "And fellow of then thy land-feel to my book a \n",
      "Epoch 24: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.4, v_num=98, test_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 82/82 [00:23<00:00,  3.44it/s, loss=1.4, v_num=98, test_loss=1.410]\n"
     ]
    }
   ],
   "source": [
    "# inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "class MinGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[DecoderLayer(embed_dim, dropout) for _ in range(num_layers)]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lm_head(self.transformer(x))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, top_k=10):\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self(input_ids[:, -block_size:])\n",
    "            logits = out[:, -1, :]\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            step_res = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            # auto-regression\n",
    "            input_ids = torch.cat((input_ids, step_res), dim=1)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = AFTFullCumsum(embed_dim, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(block_size, block_size)).to(bool).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn(x, self.mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# https://arxiv.org/pdf/2105.14103.pdf\n",
    "# but w is not exponentiated and we divide by cumulative sum of exp(k) instead of exp(w) @ exp(k)\n",
    "class AFTFullCumsum(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.wbias = nn.Parameter(torch.ones(block_size, block_size), requires_grad=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        w = self.wbias[:T,:T].masked_fill(mask[:T, :T], 0)\n",
    "        exp_k = k.exp()\n",
    "\n",
    "        weighted_avg = torch.einsum(\"ik,bkj->bij\", w, exp_k * v) / torch.cumsum(exp_k, dim=1)\n",
    "        Yt = F.sigmoid(q) * weighted_avg\n",
    "\n",
    "        return self.drop(self.proj(Yt))\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, lr=0.004):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = MinGPT(vocab_size, embed_dim=192, num_layers=3, dropout=0.1)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.model(x)\n",
    "        return logits if y is None else F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(train, batch_size=100, num_workers=0, sampler=data.RandomSampler(train, False, 5_000))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(*batch)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(test, batch_size=16, num_workers=0)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(*batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor([dataset.stoi[s] for s in \"And now \"], dtype=torch.long)[None,...].to(device)\n",
    "            y = self.model.generate(x, 64)[0]\n",
    "            print(\"\".join([dataset.itos[int(i)] for i in y]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999, last_epoch=-1)\n",
    "        return [optim], [sched]\n",
    "\n",
    "model = Model(len(dataset.vocab))\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, max_epochs=25, enable_progress_bar=True, accelerator=\"gpu\" if device == \"cuda\" else \"cpu\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! all me with the believe men!\n",
      "\n",
      "HASTINGS:\n",
      "Those mother, so must all her thou hast thorous,\n",
      "Which we shall we, to and hast she throughts,\n",
      "Forged the beloved hinolds, by thee beguil as showdit\n",
      "Fores is.\n",
      "\n",
      "First Servant:\n",
      "Say, sir; I would have the wearing and sail be specless'd;\n",
      "Then for thou these say deny again.\n",
      "\n",
      "SIMBERSAY.\n",
      "As Larcius Mowbradis, he condon him!\n",
      "The messt shall be the that's stutgar to draw the\n",
      "whereof an all, the early have langue makke, who see.\n",
      "And sail'st thy bellod faulthrow, whence\n",
      "At onthe springs mean this plaguise;\n",
      "And so help thy last that shouts me,\n",
      "And dreadly bring to depass the pever sween:\n",
      "In my lord, but think thin\n"
     ]
    }
   ],
   "source": [
    "y = model.to(device).model.generate(torch.tensor([dataset.stoi[s] for s in \"O God, O God!\"]).unsqueeze(0).to(device), 650)\n",
    "print(\"\".join([dataset.itos[int(i)] for i in y[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c070a6546767260d3817b5bdd38c64a6478cec33d40e8bb7d9bbc980115d8646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
